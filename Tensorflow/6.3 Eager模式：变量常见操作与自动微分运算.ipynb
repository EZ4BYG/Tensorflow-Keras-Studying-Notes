{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf中的**变量**指的是：在训练过程中，不断在随机梯度下降中被改变的那些量！—— 会有专门的模块、函数来收集、处理这些变量！\n",
    "- 创建张量（常量）：tf.constant()\n",
    "- 创建变量（可变）：tf.Variabel()\n",
    "\n",
    "好好区别张量的不变与变量的可变："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant( [2,3] )  # t若不是重新赋值，是不会改变的！\n",
    "v = tf.Variable( [2,3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([4, 6])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign_add( [2,3] )  # 而变量v是可以改变数值的（同一个变量地址）！\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变量的一些操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一些常见的关于变量的操作：\n",
    "- x.assign(y)：将变量x的值改变成y\n",
    "- x.assign_add(y)：将变量x的值变为x+y\n",
    "- x.read_value()：读取当前变量x的值 —— 因为它一直在变，故可以如此来查看\n",
    "- 保存网络中的值，实质就是保存网络中随着训练变化的那些变量的值 —— 后面会讲如何保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变量的自动微分计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量是如何变化的？ —— 随机梯度下降中，变量是沿着其梯度最大的方向变化的！\n",
    "- tf.GradientTape()：自动记录变量的变化 —— 其标准写法就是放在with语句中，其中命令块放待求微分的函数式！\n",
    "- 注意1：不管是变量还是常量，在求自动微分运算中，其“**值**”必须是**浮点型**的！\n",
    "- 注意2：tf.GradientTape()里面的参数若不**设置一直存储（persistent = True）**，则求完一次微分后，tf.GradientTape()磁带会自动释放，即不能再求第二次！—— 若不设置，会报Runtime的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=50, shape=(), dtype=float32, numpy=2.5555556>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于“变量”的自动微分运算：下面是tf.GradientTape的标准写法 —— with语句是一个上下文管理器\n",
    "w = tf.Variable(1.5)  # 数值必须是浮点型！ \n",
    "\n",
    "with tf.GradientTape() as t:  # tf.GradientTape()会一直跟踪记录变量t\n",
    "    loss = w * w + 1 / w      # 待求微分的函数式\n",
    "    \n",
    "grad = t.gradient(loss, w)  # 前一个参数是待求微分的函数名，后一个参数是对谁求微分\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=72, shape=(), dtype=float32, numpy=5.888889>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于“常量”的自动微分运算：多一个watch！\n",
    "w1 = tf.constant(3.0)  # 数值必须是浮点型！\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(w1)\n",
    "    loss = w1 * w1 + 1 / w1\n",
    "    \n",
    "grad1 = t.gradient(loss, w1)\n",
    "grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 想多次求微分的设置：变量、常量均适用\n",
    "w2 = tf.Variable(3.2)\n",
    "\n",
    "with tf.GradientTape( persistent = True ) as t:  # 持续存储设置打开！\n",
    "    loss1 = w2 * w2 + 1 / w2\n",
    "    loss2 = loss1 * loss1    # 多个函数 \n",
    "    \n",
    "grad1 = t.gradient( loss1, w2 )\n",
    "grad2 = t.gradient( loss2, w2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=104, shape=(), dtype=float32, numpy=6.302344>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=124, shape=(), dtype=float32, numpy=133.01097>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
