{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据输入：tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集：只用训练集即可\n",
    "(train_image, train_label), (test_image, test_label) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 28, 28, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 维度拓展，从数组转为图像尺寸\n",
    "train_image = tf.expand_dims(train_image, -1)\n",
    "test_image = tf.expand_dims(test_image, -1)\n",
    "train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换数据类型：\n",
    "train_image = tf.cast( train_image/255, tf.float32 )  # 归一化后，转为float32\n",
    "train_label = tf.cast( train_label, tf.int32 )\n",
    "\n",
    "test_image = tf.cast( test_image/255, tf.float32 )\n",
    "test_label = tf.cast( test_label, tf.int32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.data进行数据集输入：\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices( (train_image,train_label) )\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices( (test_image,test_label) )\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集乱序、分批次：\n",
    "train_dataset = train_dataset.shuffle(60000).batch(64)\n",
    "test_dataset = test_dataset.shuffle(10000).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络搭建：tf.keras.xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential( [\n",
    "    tf.keras.layers.Conv2D( 16, [3,3], activation = 'relu', input_shape = (None, None, 1) ),  # input_shape这样写即任何输入尺寸都行！\n",
    "    tf.keras.layers.Conv2D( 32, [3,3], activation = 'relu' ),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10)   #  其实可以不用softmax激活到[0,1]概率值的，哪个分数最高就是哪个！ \n",
    "] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义优化器对象：\n",
    "optimizer = tf.keras.optimizers.Adam( lr = 0.001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义损失函数对象：\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy( from_logits = True )  # 前面Dense没有用softmax激活，这样要告诉一下！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义指标计算“对象”：目标是求每个batch的！！ —— 故：加到train_step里。—— 公用对象，一直在变！！！★★★\n",
    "train_loss = tf.keras.metrics.Mean('train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_acc')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean('test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义训练函数、指标计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没用：\n",
    "# 定义损失函数：\n",
    "# 输入：模型、训练图像、真实标签；功能：计算网络对该图像的预测值（预测标签）；返回：预测值与真实值间的损失\n",
    "# def loss(model, images, labels_real):\n",
    "#     labels_predict = model(images)  # 网络的预测\n",
    "#     return loss_func(labels_real, labels_predict)  # 返回二者的交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个batch的训练函数：\n",
    "def train_step(model, images, labels_real):\n",
    "    \n",
    "    with tf.GradientTape() as t:  # 定义t要跟踪哪些函数的梯度变化！\n",
    "        labels_predict = model(images)                      # 当前model对图像的预测\n",
    "        loss_step = loss_func(labels_real, labels_predict)  # 直接用\n",
    "        \n",
    "    # 梯度优化：\n",
    "    grads = t.gradient( loss_step, model.trainable_variables )  # 求目标函数关于“各个”可训练参数的梯度值！\n",
    "    # 用上一步得到“各个”可训练参数梯度值，修改“各个”可训练参数，即也就修改了model\n",
    "    optimizer.apply_gradients( zip(grads, model.trainable_variables) )  \n",
    "    \n",
    "    # 每个batch指标计算：64张训练图片的评价loss和acc —— 公用可调用对象，一直在变！★★★\n",
    "    train_loss( loss_step )  # 每个batch中所有图的误差的均值\n",
    "    train_accuracy( labels_real, labels_predict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般每次epoch测试一次就行了，不用每个batch那么频繁！\n",
    "def test_step(model, images, labels_real):\n",
    "    labels_predict = model(images)\n",
    "    loss_step = loss_func(labels_real, labels_predict)\n",
    "    \n",
    "    # 每个batch指标计算：64张测试图片的评价loss和acc —— 公用可调用对象，一直在变！★★★\n",
    "    test_loss( loss_step )\n",
    "    test_accuracy( labels_real, labels_predict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总体训练的执行函数：注函数！\n",
    "def train():\n",
    "    \n",
    "    # 每个epoch大循环：\n",
    "    for epoch in range(10):\n",
    "        # 每个batch小循环：\n",
    "        for (batch, (images, labels_real)) in enumerate(train_dataset):  # 每次拿一个单位（batch）出来，带上编号\n",
    "            train_step( model, images, labels_real )  # 每个batch的训练！\n",
    "            # 在for循环内，也就是每个batch都在打印！\n",
    "            print( 'Epoch{}_batch{}：train_loss：{}，train_acc：{}'.format(epoch+1, \n",
    "                                                                           batch, \n",
    "                                                                           train_loss.result().numpy(), \n",
    "                                                                           train_accuracy.result().numpy()) )\n",
    "            \n",
    "        # 一直在累积：内部for循环每完毕一次，就是一个epoch完成，打印\n",
    "        print('Epoch{}：train_loss：{}，train_acc：{}'.format(epoch+1,\n",
    "                                                              train_loss.result().numpy(), \n",
    "                                                              train_accuracy.result().numpy()) )\n",
    "        \n",
    "        # 每个epoch测试一下当前模型“预测”能力：\n",
    "        for (batch, (images, labels_real)) in enumerate(test_dataset):\n",
    "            test_step( model, images, labels_real )\n",
    "        print('Epoch{}：test_loss：{}，test_acc：{}'.format(epoch+1,\n",
    "                                                            test_loss.result().numpy(),\n",
    "                                                            test_accuracy.result().numpy()) )\n",
    "        \n",
    "        # 每个epoch所有训练集完毕，需要把对象重置：\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1_batch0：train_loss：2.1725924015045166，train_acc：0.19694368541240692\n",
      "Epoch1_batch1：train_loss：2.1721155643463135，train_acc：0.1969662457704544\n",
      "Epoch1_batch2：train_loss：2.171412706375122，train_acc：0.19727273285388947\n",
      "Epoch1_batch3：train_loss：2.1705548763275146，train_acc：0.19774682819843292\n",
      "Epoch1_batch4：train_loss：2.1698434352874756，train_acc：0.1977098435163498\n",
      "Epoch1_batch5：train_loss：2.1692054271698，train_acc：0.19801034033298492\n",
      "Epoch1_batch6：train_loss：2.1683592796325684，train_acc：0.19858871400356293\n",
      "Epoch1_batch7：train_loss：2.167645215988159，train_acc：0.19899553060531616\n",
      "Epoch1_batch8：train_loss：2.1665189266204834，train_acc：0.19934386014938354\n",
      "Epoch1_batch9：train_loss：2.165571928024292，train_acc：0.19968971610069275\n",
      "Epoch1_batch10：train_loss：2.1647560596466064，train_acc：0.19975706934928894\n",
      "Epoch1_batch11：train_loss：2.163816452026367，train_acc：0.1999339759349823\n",
      "Epoch1_batch12：train_loss：2.1627390384674072，train_acc：0.2003837674856186\n",
      "Epoch1_batch13：train_loss：2.161750316619873，train_acc：0.2007211595773697\n",
      "Epoch1_batch14：train_loss：2.1606695652008057，train_acc：0.20121951401233673\n",
      "Epoch1_batch15：train_loss：2.1597869396209717，train_acc：0.2016059011220932\n",
      "Epoch1_batch16：train_loss：2.1586873531341553，train_acc：0.20204368233680725\n",
      "Epoch1_batch17：train_loss：2.1580989360809326，train_acc：0.20231680572032928\n",
      "Epoch1_batch18：train_loss：2.1572322845458984，train_acc：0.20264175534248352\n",
      "Epoch1_batch19：train_loss：2.1565661430358887，train_acc：0.203125\n",
      "Epoch1_batch20：train_loss：2.1555497646331787，train_acc：0.2033916413784027\n",
      "Epoch1_batch21：train_loss：2.154320240020752，train_acc：0.20397533476352692\n",
      "Epoch1_batch22：train_loss：2.153449296951294，train_acc：0.2042902559041977\n",
      "Epoch1_batch23：train_loss：2.152785539627075，train_acc：0.20444467663764954\n",
      "Epoch1_batch24：train_loss：2.1514856815338135，train_acc：0.20491372048854828\n",
      "Epoch1_batch25：train_loss：2.1504342555999756，train_acc：0.20522230863571167\n",
      "Epoch1_batch26：train_loss：2.149853467941284，train_acc：0.20526756346225739\n",
      "Epoch1_batch27：train_loss：2.1489081382751465，train_acc：0.20546874403953552\n",
      "Epoch1_batch28：train_loss：2.1484808921813965，train_acc：0.20540904998779297\n",
      "Epoch1_batch29：train_loss：2.1474990844726562，train_acc：0.20545323193073273\n",
      "Epoch1_batch30：train_loss：2.146432638168335，train_acc：0.20575495064258575\n",
      "Epoch1_batch31：train_loss：2.1457860469818115，train_acc：0.20600329339504242\n",
      "Epoch1_batch32：train_loss：2.145010232925415，train_acc：0.20624999701976776\n",
      "Epoch1_batch33：train_loss：2.144577980041504，train_acc：0.20644403994083405\n",
      "Epoch1_batch34：train_loss：2.1437771320343018，train_acc：0.20653501152992249\n",
      "Epoch1_batch35：train_loss：2.1433184146881104，train_acc：0.2064732164144516\n",
      "Epoch1_batch36：train_loss：2.1422359943389893，train_acc：0.2069680392742157\n",
      "Epoch1_batch37：train_loss：2.1412341594696045，train_acc：0.2075100839138031\n",
      "Epoch1_batch38：train_loss：2.14052677154541，train_acc：0.20789791643619537\n",
      "Epoch1_batch39：train_loss：2.139439821243286，train_acc：0.20868389308452606\n",
      "Epoch1_batch40：train_loss：2.138246536254883，train_acc：0.20921525359153748\n",
      "Epoch1_batch41：train_loss：2.137279748916626，train_acc：0.20964370667934418\n",
      "Epoch1_batch42：train_loss：2.136518955230713，train_acc：0.20987103879451752\n",
      "Epoch1_batch43：train_loss：2.135683059692383，train_acc：0.2102946937084198\n",
      "Epoch1_batch44：train_loss：2.134737253189087，train_acc：0.21051853895187378\n",
      "Epoch1_batch45：train_loss：2.133690357208252，train_acc：0.21103577315807343\n",
      "Epoch1_batch46：train_loss：2.1325790882110596，train_acc：0.21150077879428864\n",
      "Epoch1_batch47：train_loss：2.1315393447875977，train_acc：0.21206054091453552\n",
      "Epoch1_batch48：train_loss：2.1306986808776855，train_acc：0.21232476830482483\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-c53202dab8f0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# 每个batch小循环：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 每次拿一个单位（batch）出来，带上编号\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_real\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;31m# 每个batch的训练！\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;31m# 在for循环内，也就是每个batch都在打印！\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             print( 'Epoch{}_batch{}：train_loss：{}，train_acc：{}'.format(epoch+1, \n",
      "\u001b[1;32m<ipython-input-56-f64e3cc60386>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, images, labels_real)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# 梯度优化：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mloss_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;31m# 求目标函数关于“各个”可训练参数的梯度值！\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# 用上一步得到“各个”可训练参数梯度值，修改“各个”可训练参数，即也就修改了model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Pycharm\\python374\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Pycharm\\python374\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mE:\\Pycharm\\python374\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Pycharm\\python374\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m           data_format=data_format)\n\u001b[0m\u001b[0;32m    607\u001b[0m   ]\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Pycharm\\python374\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明的所有自定义函数，其功能完成已经和tf.keras封装好的高级操作一样了！\n",
    "- tf.keras会用3大模块：tf.keras.layers、model.compile、model.fit\n",
    "- 自定义用到的可调用对象：optimizer、loss_func；train_loss、train_accuracy；test_func、test_accuracy\n",
    "- 自定义用到的函数：train_step、test_step、train\n",
    "\n",
    "注意\n",
    "- 在使用自定义搭建时，用实例化函数创建的都是“**可调用对象**”！！例如：optimizer、loss_func；train_loss、train_accuracy；test_func、test_accuracy都是！—— 不论何时使用，它们都有“**自动记录**”功能！\n",
    "- 用tf.keras高阶模块搭建时，dataset数据需要.repeat()；当是自定义循环时，不需要用.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
