{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一共有990个数据，其中有99种叶子。每个叶子都有192个特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('E:/Python_code/keras_total/日月光华-keras课程资料/小型数据集/leaf/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                species   margin1   margin2   margin3   margin4  \\\n",
       "0   1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1   2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2   3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3   5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4   6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "\n",
       "    margin5   margin6   margin7  margin8  ...  texture55  texture56  \\\n",
       "0  0.011719  0.009766  0.027344      0.0  ...   0.007812   0.000000   \n",
       "1  0.025391  0.001953  0.019531      0.0  ...   0.000977   0.000000   \n",
       "2  0.003906  0.005859  0.068359      0.0  ...   0.154300   0.000000   \n",
       "3  0.021484  0.019531  0.023438      0.0  ...   0.000000   0.000977   \n",
       "4  0.013672  0.015625  0.005859      0.0  ...   0.096680   0.000000   \n",
       "\n",
       "   texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0   0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
       "1   0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
       "2   0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
       "3   0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
       "4   0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
       "\n",
       "   texture63  texture64  \n",
       "0   0.000000   0.025391  \n",
       "1   0.039062   0.022461  \n",
       "2   0.020508   0.002930  \n",
       "3   0.000000   0.047852  \n",
       "4   0.000000   0.031250  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 990 entries, 0 to 989\n",
      "Columns: 194 entries, id to texture64\n",
      "dtypes: float64(192), int64(1), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "# 很干净的数据，中间没有空缺、没有杂项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# 一共有990个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.species.unique())\n",
    "# 一共有99种叶子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.factorize(data.species)[0]  # 直接把叶子种类进行“顺序编码(1, 2, 3, ... 99)”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[data.columns[2:]]  # 直接把192个特征列提取出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接用sklearn进行数据分割：自动非训练数据 + 测试数据；训练数据标签 + 测试数据标签\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((742, 192), (248, 192))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行标准化： 减均值、除方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_x.mean(axis=0)\n",
    "std = train_x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = (train_x - mean)/std\n",
    "test_x = (test_x - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据调整："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把每一条数据看成一个序列，一维卷积或者LSTM的数据都是3维的：(samples, step, feature)\n",
    "\n",
    "现在的step长度，就是那192个特征，feature就是那最后的一个标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把维度调整为3维：\n",
    "train_x = np.expand_dims(train_x, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(742, 192, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.expand_dims(test_x, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络搭建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层网络：比如设置输入数据的尺寸\n",
    "model.add( layers.Conv1D(32, 7, input_shape = (train_x.shape[1:]), activation = 'relu', padding = 'same' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\pycharm\\python374\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add( layers.MaxPooling1D(3) )  # 数据长度缩小3倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add( layers.Conv1D(32, 7, activation = 'relu', padding = 'same' ) )\n",
    "model.add( layers.GlobalAveragePooling1D() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 192, 32)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 64, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 32)                0         \n",
      "=================================================================\n",
      "Total params: 7,456\n",
      "Trainable params: 7,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add( layers.Dense(99, activation='softmax') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 192, 32)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 64, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 99)                3267      \n",
      "=================================================================\n",
      "Total params: 10,723\n",
      "Trainable params: 10,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop优化器，非常善于处理序列问题；\n",
    "# 目标值是“顺序编码”的多分类，所以loss用：sparse_categorical_crossentropy\n",
    "model.compile( optimizer = keras.optimizers.RMSprop(),\n",
    "               loss = 'sparse_categorical_crossentropy',\n",
    "               metrics = ['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 742 samples, validate on 248 samples\n",
      "Epoch 1/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.7227 - acc: 0.7978 - val_loss: 1.2240 - val_acc: 0.6371\n",
      "Epoch 2/600\n",
      "742/742 [==============================] - 0s 269us/step - loss: 0.7107 - acc: 0.8032 - val_loss: 1.2456 - val_acc: 0.6250\n",
      "Epoch 3/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.7315 - acc: 0.7925 - val_loss: 1.2475 - val_acc: 0.6210\n",
      "Epoch 4/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.7010 - acc: 0.8100 - val_loss: 1.2178 - val_acc: 0.6290\n",
      "Epoch 5/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.7138 - acc: 0.8073 - val_loss: 1.1756 - val_acc: 0.6573\n",
      "Epoch 6/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.6984 - acc: 0.8194 - val_loss: 1.2317 - val_acc: 0.5968\n",
      "Epoch 7/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.7056 - acc: 0.8032 - val_loss: 1.2240 - val_acc: 0.6411\n",
      "Epoch 8/600\n",
      "742/742 [==============================] - 0s 267us/step - loss: 0.7032 - acc: 0.8032 - val_loss: 1.2225 - val_acc: 0.6371\n",
      "Epoch 9/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.6987 - acc: 0.8167 - val_loss: 1.2203 - val_acc: 0.6008\n",
      "Epoch 10/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.6906 - acc: 0.8059 - val_loss: 1.1882 - val_acc: 0.6452\n",
      "Epoch 11/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.6807 - acc: 0.8208 - val_loss: 1.1843 - val_acc: 0.6250\n",
      "Epoch 12/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.6949 - acc: 0.8086 - val_loss: 1.2251 - val_acc: 0.6129\n",
      "Epoch 13/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.6904 - acc: 0.8194 - val_loss: 1.2308 - val_acc: 0.5927\n",
      "Epoch 14/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.6776 - acc: 0.8181 - val_loss: 1.1886 - val_acc: 0.6452\n",
      "Epoch 15/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.6749 - acc: 0.8302 - val_loss: 1.2299 - val_acc: 0.6371\n",
      "Epoch 16/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.6742 - acc: 0.8275 - val_loss: 1.2065 - val_acc: 0.6169\n",
      "Epoch 17/600\n",
      "742/742 [==============================] - 0s 224us/step - loss: 0.6833 - acc: 0.8154 - val_loss: 1.1547 - val_acc: 0.6573\n",
      "Epoch 18/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.6819 - acc: 0.8127 - val_loss: 1.1779 - val_acc: 0.6573\n",
      "Epoch 19/600\n",
      "742/742 [==============================] - 0s 264us/step - loss: 0.6847 - acc: 0.8181 - val_loss: 1.1938 - val_acc: 0.6290\n",
      "Epoch 20/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.6655 - acc: 0.8208 - val_loss: 1.1572 - val_acc: 0.6532\n",
      "Epoch 21/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.6631 - acc: 0.8302 - val_loss: 1.1669 - val_acc: 0.6653\n",
      "Epoch 22/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.6780 - acc: 0.8167 - val_loss: 1.1527 - val_acc: 0.6573\n",
      "Epoch 23/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.6431 - acc: 0.8369 - val_loss: 1.2144 - val_acc: 0.6210\n",
      "Epoch 24/600\n",
      "742/742 [==============================] - 0s 271us/step - loss: 0.6541 - acc: 0.8181 - val_loss: 1.1710 - val_acc: 0.6210\n",
      "Epoch 25/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.6785 - acc: 0.8154 - val_loss: 1.1713 - val_acc: 0.6371\n",
      "Epoch 26/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.6663 - acc: 0.8275 - val_loss: 1.2361 - val_acc: 0.6008\n",
      "Epoch 27/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.6625 - acc: 0.8194 - val_loss: 1.2315 - val_acc: 0.6048\n",
      "Epoch 28/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.6460 - acc: 0.8208 - val_loss: 1.1776 - val_acc: 0.6331\n",
      "Epoch 29/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.6553 - acc: 0.8302 - val_loss: 1.2045 - val_acc: 0.6169\n",
      "Epoch 30/600\n",
      "742/742 [==============================] - 0s 260us/step - loss: 0.6342 - acc: 0.8437 - val_loss: 1.1488 - val_acc: 0.6492\n",
      "Epoch 31/600\n",
      "742/742 [==============================] - 0s 269us/step - loss: 0.6475 - acc: 0.8410 - val_loss: 1.2376 - val_acc: 0.6371\n",
      "Epoch 32/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.6378 - acc: 0.8396 - val_loss: 1.2655 - val_acc: 0.6129\n",
      "Epoch 33/600\n",
      "742/742 [==============================] - 0s 217us/step - loss: 0.6694 - acc: 0.8127 - val_loss: 1.2332 - val_acc: 0.6048\n",
      "Epoch 34/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.6474 - acc: 0.8423 - val_loss: 1.1652 - val_acc: 0.6371\n",
      "Epoch 35/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.6326 - acc: 0.8261 - val_loss: 1.1867 - val_acc: 0.6290\n",
      "Epoch 36/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.6476 - acc: 0.8275 - val_loss: 1.2118 - val_acc: 0.6048\n",
      "Epoch 37/600\n",
      "742/742 [==============================] - 0s 280us/step - loss: 0.6536 - acc: 0.8221 - val_loss: 1.2045 - val_acc: 0.6653\n",
      "Epoch 38/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.6306 - acc: 0.8315 - val_loss: 1.1774 - val_acc: 0.6371\n",
      "Epoch 39/600\n",
      "742/742 [==============================] - 0s 227us/step - loss: 0.6243 - acc: 0.8342 - val_loss: 1.2153 - val_acc: 0.6169\n",
      "Epoch 40/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.6196 - acc: 0.8329 - val_loss: 1.1009 - val_acc: 0.6653\n",
      "Epoch 41/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.6209 - acc: 0.8342 - val_loss: 1.1588 - val_acc: 0.6290\n",
      "Epoch 42/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.6099 - acc: 0.8288 - val_loss: 1.2271 - val_acc: 0.6411\n",
      "Epoch 43/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.6327 - acc: 0.8329 - val_loss: 1.1610 - val_acc: 0.6573\n",
      "Epoch 44/600\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.6229 - acc: 0.839 - 0s 231us/step - loss: 0.6252 - acc: 0.8369 - val_loss: 1.1967 - val_acc: 0.6331\n",
      "Epoch 45/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.6306 - acc: 0.8410 - val_loss: 1.1799 - val_acc: 0.6613\n",
      "Epoch 46/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.6268 - acc: 0.8450 - val_loss: 1.1235 - val_acc: 0.6573\n",
      "Epoch 47/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.6202 - acc: 0.8356 - val_loss: 1.1542 - val_acc: 0.6573\n",
      "Epoch 48/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.5999 - acc: 0.8464 - val_loss: 1.1226 - val_acc: 0.6532\n",
      "Epoch 49/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.6119 - acc: 0.8329 - val_loss: 1.1550 - val_acc: 0.6532\n",
      "Epoch 50/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.6243 - acc: 0.8329 - val_loss: 1.1816 - val_acc: 0.6492\n",
      "Epoch 51/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.6161 - acc: 0.8423 - val_loss: 1.1235 - val_acc: 0.6734\n",
      "Epoch 52/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.6237 - acc: 0.8208 - val_loss: 1.1032 - val_acc: 0.6694\n",
      "Epoch 53/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.6014 - acc: 0.8315 - val_loss: 1.1337 - val_acc: 0.6532\n",
      "Epoch 54/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.6031 - acc: 0.8302 - val_loss: 1.1417 - val_acc: 0.6613\n",
      "Epoch 55/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.6076 - acc: 0.8477 - val_loss: 1.1275 - val_acc: 0.6573\n",
      "Epoch 56/600\n",
      "742/742 [==============================] - 0s 262us/step - loss: 0.5989 - acc: 0.8329 - val_loss: 1.1603 - val_acc: 0.6532\n",
      "Epoch 57/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.6395 - acc: 0.8208 - val_loss: 1.1131 - val_acc: 0.6573\n",
      "Epoch 58/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.5838 - acc: 0.8491 - val_loss: 1.1452 - val_acc: 0.6774\n",
      "Epoch 59/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.5800 - acc: 0.8518 - val_loss: 1.1665 - val_acc: 0.6573\n",
      "Epoch 60/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.5893 - acc: 0.8275 - val_loss: 1.1323 - val_acc: 0.6815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.5887 - acc: 0.8477 - val_loss: 1.0731 - val_acc: 0.6694\n",
      "Epoch 62/600\n",
      "742/742 [==============================] - 0s 263us/step - loss: 0.5887 - acc: 0.8571 - val_loss: 1.1151 - val_acc: 0.6532\n",
      "Epoch 63/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.5998 - acc: 0.8383 - val_loss: 1.1469 - val_acc: 0.6250\n",
      "Epoch 64/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.5996 - acc: 0.8302 - val_loss: 1.1022 - val_acc: 0.6734\n",
      "Epoch 65/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.5708 - acc: 0.8544 - val_loss: 1.0826 - val_acc: 0.6815\n",
      "Epoch 66/600\n",
      "742/742 [==============================] - 0s 280us/step - loss: 0.5843 - acc: 0.8356 - val_loss: 1.1374 - val_acc: 0.6492\n",
      "Epoch 67/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.5698 - acc: 0.8652 - val_loss: 1.0863 - val_acc: 0.6653\n",
      "Epoch 68/600\n",
      "742/742 [==============================] - 0s 261us/step - loss: 0.5882 - acc: 0.8356 - val_loss: 1.1360 - val_acc: 0.6613\n",
      "Epoch 69/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.5559 - acc: 0.8571 - val_loss: 1.1227 - val_acc: 0.6653\n",
      "Epoch 70/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.5671 - acc: 0.8477 - val_loss: 1.1237 - val_acc: 0.6694\n",
      "Epoch 71/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.5834 - acc: 0.8464 - val_loss: 1.1903 - val_acc: 0.6129\n",
      "Epoch 72/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.5712 - acc: 0.8598 - val_loss: 1.1056 - val_acc: 0.6653\n",
      "Epoch 73/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.5760 - acc: 0.8464 - val_loss: 1.1160 - val_acc: 0.6532\n",
      "Epoch 74/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.5609 - acc: 0.8518 - val_loss: 1.1506 - val_acc: 0.6532\n",
      "Epoch 75/600\n",
      "742/742 [==============================] - 0s 218us/step - loss: 0.5697 - acc: 0.8464 - val_loss: 1.0825 - val_acc: 0.6734\n",
      "Epoch 76/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.5713 - acc: 0.8477 - val_loss: 1.0974 - val_acc: 0.6694\n",
      "Epoch 77/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.5517 - acc: 0.8518 - val_loss: 1.0571 - val_acc: 0.6774\n",
      "Epoch 78/600\n",
      "742/742 [==============================] - 0s 255us/step - loss: 0.5496 - acc: 0.8585 - val_loss: 1.0823 - val_acc: 0.6653\n",
      "Epoch 79/600\n",
      "742/742 [==============================] - 0s 258us/step - loss: 0.5613 - acc: 0.8491 - val_loss: 1.1167 - val_acc: 0.6774\n",
      "Epoch 80/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.5666 - acc: 0.8450 - val_loss: 1.1075 - val_acc: 0.6895\n",
      "Epoch 81/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.5586 - acc: 0.8491 - val_loss: 1.0978 - val_acc: 0.6895\n",
      "Epoch 82/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.5496 - acc: 0.8585 - val_loss: 1.1566 - val_acc: 0.6613\n",
      "Epoch 83/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.5494 - acc: 0.8518 - val_loss: 1.1219 - val_acc: 0.6573\n",
      "Epoch 84/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.5275 - acc: 0.8666 - val_loss: 1.1407 - val_acc: 0.6573\n",
      "Epoch 85/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.5474 - acc: 0.8598 - val_loss: 1.0839 - val_acc: 0.6452\n",
      "Epoch 86/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.5426 - acc: 0.8598 - val_loss: 1.1385 - val_acc: 0.6411\n",
      "Epoch 87/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.5479 - acc: 0.8558 - val_loss: 1.0634 - val_acc: 0.6774\n",
      "Epoch 88/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.5242 - acc: 0.8720 - val_loss: 1.0938 - val_acc: 0.6532\n",
      "Epoch 89/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.5425 - acc: 0.8598 - val_loss: 1.0773 - val_acc: 0.6855\n",
      "Epoch 90/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.5484 - acc: 0.8491 - val_loss: 1.1215 - val_acc: 0.6653\n",
      "Epoch 91/600\n",
      "742/742 [==============================] - 0s 212us/step - loss: 0.5313 - acc: 0.8558 - val_loss: 1.0992 - val_acc: 0.6532\n",
      "Epoch 92/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.5340 - acc: 0.8679 - val_loss: 1.1292 - val_acc: 0.6573\n",
      "Epoch 93/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.5208 - acc: 0.8706 - val_loss: 1.0950 - val_acc: 0.6734\n",
      "Epoch 94/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.5322 - acc: 0.8652 - val_loss: 1.1543 - val_acc: 0.6734\n",
      "Epoch 95/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.5560 - acc: 0.8504 - val_loss: 1.1054 - val_acc: 0.6532\n",
      "Epoch 96/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.5211 - acc: 0.8666 - val_loss: 1.0747 - val_acc: 0.6694\n",
      "Epoch 97/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.5185 - acc: 0.8733 - val_loss: 1.1219 - val_acc: 0.6653\n",
      "Epoch 98/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.5439 - acc: 0.8450 - val_loss: 1.0204 - val_acc: 0.7097\n",
      "Epoch 99/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.5098 - acc: 0.8639 - val_loss: 1.1459 - val_acc: 0.6855\n",
      "Epoch 100/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.5239 - acc: 0.8612 - val_loss: 1.0580 - val_acc: 0.6895\n",
      "Epoch 101/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.5213 - acc: 0.8612 - val_loss: 1.0385 - val_acc: 0.6895\n",
      "Epoch 102/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.5293 - acc: 0.8598 - val_loss: 1.1109 - val_acc: 0.6694\n",
      "Epoch 103/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.5162 - acc: 0.8612 - val_loss: 1.0977 - val_acc: 0.6653\n",
      "Epoch 104/600\n",
      "742/742 [==============================] - 0s 214us/step - loss: 0.5035 - acc: 0.8841 - val_loss: 1.0656 - val_acc: 0.6774\n",
      "Epoch 105/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.5302 - acc: 0.8612 - val_loss: 1.0947 - val_acc: 0.6855\n",
      "Epoch 106/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.4950 - acc: 0.8625 - val_loss: 1.0482 - val_acc: 0.6734\n",
      "Epoch 107/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.5096 - acc: 0.8747 - val_loss: 1.0540 - val_acc: 0.6815\n",
      "Epoch 108/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.4858 - acc: 0.8814 - val_loss: 1.1186 - val_acc: 0.6492\n",
      "Epoch 109/600\n",
      "742/742 [==============================] - 0s 262us/step - loss: 0.5040 - acc: 0.8693 - val_loss: 1.1176 - val_acc: 0.6573\n",
      "Epoch 110/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.4928 - acc: 0.8760 - val_loss: 1.0552 - val_acc: 0.6935\n",
      "Epoch 111/600\n",
      "742/742 [==============================] - 0s 259us/step - loss: 0.5195 - acc: 0.8679 - val_loss: 1.0647 - val_acc: 0.6532\n",
      "Epoch 112/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.4966 - acc: 0.8720 - val_loss: 1.0501 - val_acc: 0.6694\n",
      "Epoch 113/600\n",
      "742/742 [==============================] - 0s 264us/step - loss: 0.4938 - acc: 0.8652 - val_loss: 1.0975 - val_acc: 0.6815\n",
      "Epoch 114/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.5000 - acc: 0.8679 - val_loss: 1.0111 - val_acc: 0.7016\n",
      "Epoch 115/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.4936 - acc: 0.8679 - val_loss: 1.0233 - val_acc: 0.7137\n",
      "Epoch 116/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4827 - acc: 0.8868 - val_loss: 1.0705 - val_acc: 0.6855\n",
      "Epoch 117/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4865 - acc: 0.8747 - val_loss: 1.0508 - val_acc: 0.6855\n",
      "Epoch 118/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.5057 - acc: 0.8625 - val_loss: 1.0638 - val_acc: 0.7016\n",
      "Epoch 119/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.4869 - acc: 0.8760 - val_loss: 1.0726 - val_acc: 0.6976\n",
      "Epoch 120/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.4993 - acc: 0.8625 - val_loss: 1.0500 - val_acc: 0.7298\n",
      "Epoch 121/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 252us/step - loss: 0.4729 - acc: 0.8720 - val_loss: 1.0098 - val_acc: 0.6895\n",
      "Epoch 122/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.4859 - acc: 0.8706 - val_loss: 1.1314 - val_acc: 0.6452\n",
      "Epoch 123/600\n",
      "742/742 [==============================] - 0s 227us/step - loss: 0.4985 - acc: 0.8585 - val_loss: 1.0334 - val_acc: 0.6573\n",
      "Epoch 124/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.4834 - acc: 0.8720 - val_loss: 1.0280 - val_acc: 0.7016\n",
      "Epoch 125/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.4650 - acc: 0.8827 - val_loss: 1.1079 - val_acc: 0.6734\n",
      "Epoch 126/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.5005 - acc: 0.8410 - val_loss: 1.1294 - val_acc: 0.6331\n",
      "Epoch 127/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.4885 - acc: 0.8693 - val_loss: 1.0386 - val_acc: 0.6694\n",
      "Epoch 128/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.4568 - acc: 0.8814 - val_loss: 0.9766 - val_acc: 0.7056\n",
      "Epoch 129/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.4801 - acc: 0.8774 - val_loss: 1.0512 - val_acc: 0.6895\n",
      "Epoch 130/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.4834 - acc: 0.8720 - val_loss: 1.0913 - val_acc: 0.6573\n",
      "Epoch 131/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.4693 - acc: 0.8801 - val_loss: 1.0101 - val_acc: 0.6774\n",
      "Epoch 132/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.4595 - acc: 0.8935 - val_loss: 1.0157 - val_acc: 0.6935\n",
      "Epoch 133/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4839 - acc: 0.8760 - val_loss: 1.0360 - val_acc: 0.6976\n",
      "Epoch 134/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.4817 - acc: 0.8652 - val_loss: 1.0218 - val_acc: 0.7137\n",
      "Epoch 135/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.4564 - acc: 0.8787 - val_loss: 0.9600 - val_acc: 0.7137\n",
      "Epoch 136/600\n",
      "742/742 [==============================] - 0s 209us/step - loss: 0.4529 - acc: 0.8908 - val_loss: 1.0537 - val_acc: 0.6855\n",
      "Epoch 137/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.4554 - acc: 0.8827 - val_loss: 1.0733 - val_acc: 0.6935\n",
      "Epoch 138/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.4707 - acc: 0.8814 - val_loss: 1.0282 - val_acc: 0.7218\n",
      "Epoch 139/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.4510 - acc: 0.8881 - val_loss: 1.0158 - val_acc: 0.7056\n",
      "Epoch 140/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.4486 - acc: 0.8774 - val_loss: 1.1220 - val_acc: 0.6573\n",
      "Epoch 141/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.4669 - acc: 0.8827 - val_loss: 1.0596 - val_acc: 0.6532\n",
      "Epoch 142/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4673 - acc: 0.8827 - val_loss: 1.0198 - val_acc: 0.6935\n",
      "Epoch 143/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.4776 - acc: 0.8625 - val_loss: 1.0701 - val_acc: 0.6935\n",
      "Epoch 144/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.4544 - acc: 0.8881 - val_loss: 1.0171 - val_acc: 0.6855\n",
      "Epoch 145/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.4408 - acc: 0.8881 - val_loss: 1.0347 - val_acc: 0.7016\n",
      "Epoch 146/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.4395 - acc: 0.8881 - val_loss: 0.9571 - val_acc: 0.7016\n",
      "Epoch 147/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.4388 - acc: 0.8962 - val_loss: 1.0026 - val_acc: 0.7097\n",
      "Epoch 148/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.4496 - acc: 0.8747 - val_loss: 0.9962 - val_acc: 0.6774\n",
      "Epoch 149/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4284 - acc: 0.9057 - val_loss: 1.0888 - val_acc: 0.6815\n",
      "Epoch 150/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.4483 - acc: 0.8881 - val_loss: 1.0565 - val_acc: 0.6895\n",
      "Epoch 151/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.4412 - acc: 0.8733 - val_loss: 1.0550 - val_acc: 0.6855\n",
      "Epoch 152/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.4459 - acc: 0.8801 - val_loss: 1.0499 - val_acc: 0.7177\n",
      "Epoch 153/600\n",
      "742/742 [==============================] - 0s 209us/step - loss: 0.4463 - acc: 0.8706 - val_loss: 1.0550 - val_acc: 0.6653\n",
      "Epoch 154/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.4386 - acc: 0.8868 - val_loss: 1.0704 - val_acc: 0.6734\n",
      "Epoch 155/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.4484 - acc: 0.8787 - val_loss: 0.9803 - val_acc: 0.6976\n",
      "Epoch 156/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.4289 - acc: 0.9016 - val_loss: 1.0566 - val_acc: 0.6895\n",
      "Epoch 157/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.4268 - acc: 0.8922 - val_loss: 0.9846 - val_acc: 0.7258\n",
      "Epoch 158/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.4147 - acc: 0.9057 - val_loss: 1.0401 - val_acc: 0.6895\n",
      "Epoch 159/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4352 - acc: 0.8881 - val_loss: 0.9886 - val_acc: 0.6935\n",
      "Epoch 160/600\n",
      "742/742 [==============================] - 0s 215us/step - loss: 0.4341 - acc: 0.8881 - val_loss: 1.0617 - val_acc: 0.6935\n",
      "Epoch 161/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.4366 - acc: 0.8881 - val_loss: 0.9941 - val_acc: 0.7177\n",
      "Epoch 162/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.4195 - acc: 0.8962 - val_loss: 1.0212 - val_acc: 0.6976\n",
      "Epoch 163/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.4191 - acc: 0.8935 - val_loss: 0.9808 - val_acc: 0.6976\n",
      "Epoch 164/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.4378 - acc: 0.8922 - val_loss: 1.0732 - val_acc: 0.6774\n",
      "Epoch 165/600\n",
      "742/742 [==============================] - 0s 210us/step - loss: 0.4105 - acc: 0.8935 - val_loss: 0.9665 - val_acc: 0.7298\n",
      "Epoch 166/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.4195 - acc: 0.8962 - val_loss: 1.0021 - val_acc: 0.7056\n",
      "Epoch 167/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.4064 - acc: 0.9097 - val_loss: 1.0307 - val_acc: 0.6895\n",
      "Epoch 168/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.4113 - acc: 0.8976 - val_loss: 1.0441 - val_acc: 0.6855\n",
      "Epoch 169/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.4051 - acc: 0.8989 - val_loss: 0.9419 - val_acc: 0.7137\n",
      "Epoch 170/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.4229 - acc: 0.8935 - val_loss: 1.0621 - val_acc: 0.6694\n",
      "Epoch 171/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.4159 - acc: 0.8908 - val_loss: 0.9596 - val_acc: 0.7137\n",
      "Epoch 172/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.4142 - acc: 0.9043 - val_loss: 1.0229 - val_acc: 0.6613\n",
      "Epoch 173/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.4086 - acc: 0.9016 - val_loss: 0.9936 - val_acc: 0.6976\n",
      "Epoch 174/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.4144 - acc: 0.9030 - val_loss: 0.9571 - val_acc: 0.7097\n",
      "Epoch 175/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4035 - acc: 0.9043 - val_loss: 1.0012 - val_acc: 0.6774\n",
      "Epoch 176/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.3824 - acc: 0.9137 - val_loss: 1.0252 - val_acc: 0.6895\n",
      "Epoch 177/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.4017 - acc: 0.9043 - val_loss: 1.0351 - val_acc: 0.6653\n",
      "Epoch 178/600\n",
      "742/742 [==============================] - 0s 258us/step - loss: 0.4021 - acc: 0.9057 - val_loss: 0.9698 - val_acc: 0.7016\n",
      "Epoch 179/600\n",
      "742/742 [==============================] - 0s 213us/step - loss: 0.4086 - acc: 0.8895 - val_loss: 1.0181 - val_acc: 0.7097\n",
      "Epoch 180/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.3988 - acc: 0.9030 - val_loss: 0.9939 - val_acc: 0.7016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.3948 - acc: 0.9151 - val_loss: 0.9874 - val_acc: 0.6976\n",
      "Epoch 182/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.4041 - acc: 0.8976 - val_loss: 1.0654 - val_acc: 0.6855\n",
      "Epoch 183/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.3950 - acc: 0.8976 - val_loss: 1.0161 - val_acc: 0.6855\n",
      "Epoch 184/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.3918 - acc: 0.9111 - val_loss: 0.9992 - val_acc: 0.7097\n",
      "Epoch 185/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.3889 - acc: 0.9030 - val_loss: 0.9737 - val_acc: 0.6855\n",
      "Epoch 186/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.3964 - acc: 0.9003 - val_loss: 1.0166 - val_acc: 0.7056\n",
      "Epoch 187/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.4151 - acc: 0.8774 - val_loss: 1.0456 - val_acc: 0.7177\n",
      "Epoch 188/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.3854 - acc: 0.9097 - val_loss: 0.9623 - val_acc: 0.7016\n",
      "Epoch 189/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.3850 - acc: 0.9003 - val_loss: 0.9551 - val_acc: 0.7097\n",
      "Epoch 190/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.3934 - acc: 0.9016 - val_loss: 0.9838 - val_acc: 0.7016\n",
      "Epoch 191/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.3953 - acc: 0.8895 - val_loss: 0.9784 - val_acc: 0.7097\n",
      "Epoch 192/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.3827 - acc: 0.9137 - val_loss: 0.9513 - val_acc: 0.7097\n",
      "Epoch 193/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.3911 - acc: 0.8962 - val_loss: 0.9415 - val_acc: 0.7177\n",
      "Epoch 194/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.3836 - acc: 0.9016 - val_loss: 0.9901 - val_acc: 0.7137\n",
      "Epoch 195/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.3775 - acc: 0.9164 - val_loss: 0.9880 - val_acc: 0.7016\n",
      "Epoch 196/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.3717 - acc: 0.9097 - val_loss: 1.0036 - val_acc: 0.6855\n",
      "Epoch 197/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.3640 - acc: 0.9286 - val_loss: 1.0194 - val_acc: 0.7379\n",
      "Epoch 198/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.3947 - acc: 0.8868 - val_loss: 0.9596 - val_acc: 0.7339\n",
      "Epoch 199/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.3805 - acc: 0.9084 - val_loss: 0.9852 - val_acc: 0.6694\n",
      "Epoch 200/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.3796 - acc: 0.9003 - val_loss: 1.0055 - val_acc: 0.6694\n",
      "Epoch 201/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.3794 - acc: 0.9030 - val_loss: 0.9391 - val_acc: 0.6976\n",
      "Epoch 202/600\n",
      "742/742 [==============================] - 0s 213us/step - loss: 0.3639 - acc: 0.9097 - val_loss: 1.0172 - val_acc: 0.6935\n",
      "Epoch 203/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.3574 - acc: 0.9137 - val_loss: 0.9516 - val_acc: 0.7097\n",
      "Epoch 204/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.3747 - acc: 0.9097 - val_loss: 0.9670 - val_acc: 0.7298\n",
      "Epoch 205/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.3803 - acc: 0.9070 - val_loss: 0.9731 - val_acc: 0.7056\n",
      "Epoch 206/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.3578 - acc: 0.9084 - val_loss: 0.9793 - val_acc: 0.7056\n",
      "Epoch 207/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.3557 - acc: 0.9191 - val_loss: 1.0145 - val_acc: 0.6855\n",
      "Epoch 208/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.3716 - acc: 0.9016 - val_loss: 1.0189 - val_acc: 0.7097\n",
      "Epoch 209/600\n",
      "742/742 [==============================] - 0s 266us/step - loss: 0.3763 - acc: 0.9137 - val_loss: 0.9691 - val_acc: 0.7258\n",
      "Epoch 210/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.3589 - acc: 0.9070 - val_loss: 0.9828 - val_acc: 0.7056\n",
      "Epoch 211/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.3462 - acc: 0.9218 - val_loss: 0.9726 - val_acc: 0.7016\n",
      "Epoch 212/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.3660 - acc: 0.9016 - val_loss: 0.9621 - val_acc: 0.7298\n",
      "Epoch 213/600\n",
      "742/742 [==============================] - 0s 225us/step - loss: 0.3550 - acc: 0.9124 - val_loss: 0.9729 - val_acc: 0.7218\n",
      "Epoch 214/600\n",
      "742/742 [==============================] - 0s 219us/step - loss: 0.3544 - acc: 0.9245 - val_loss: 1.0161 - val_acc: 0.7056\n",
      "Epoch 215/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.3704 - acc: 0.9097 - val_loss: 0.9873 - val_acc: 0.6774\n",
      "Epoch 216/600\n",
      "742/742 [==============================] - 0s 216us/step - loss: 0.3541 - acc: 0.9191 - val_loss: 0.9966 - val_acc: 0.7016\n",
      "Epoch 217/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.3543 - acc: 0.9124 - val_loss: 0.9550 - val_acc: 0.7016\n",
      "Epoch 218/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.3470 - acc: 0.9124 - val_loss: 0.9576 - val_acc: 0.7137\n",
      "Epoch 219/600\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.3505 - acc: 0.925 - 0s 246us/step - loss: 0.3581 - acc: 0.9191 - val_loss: 0.9984 - val_acc: 0.7137\n",
      "Epoch 220/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.3548 - acc: 0.9070 - val_loss: 0.9675 - val_acc: 0.7258\n",
      "Epoch 221/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.3607 - acc: 0.9111 - val_loss: 0.9695 - val_acc: 0.7097\n",
      "Epoch 222/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.3448 - acc: 0.9205 - val_loss: 0.9581 - val_acc: 0.7339\n",
      "Epoch 223/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.3495 - acc: 0.9232 - val_loss: 0.9632 - val_acc: 0.7419\n",
      "Epoch 224/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.3401 - acc: 0.9205 - val_loss: 0.9715 - val_acc: 0.7258\n",
      "Epoch 225/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.3465 - acc: 0.9124 - val_loss: 0.9248 - val_acc: 0.7056\n",
      "Epoch 226/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.3591 - acc: 0.9137 - val_loss: 0.9963 - val_acc: 0.7097\n",
      "Epoch 227/600\n",
      "742/742 [==============================] - 0s 258us/step - loss: 0.3354 - acc: 0.9178 - val_loss: 0.9782 - val_acc: 0.7056\n",
      "Epoch 228/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.3447 - acc: 0.9097 - val_loss: 0.9714 - val_acc: 0.6976\n",
      "Epoch 229/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.3461 - acc: 0.9111 - val_loss: 0.9707 - val_acc: 0.7097\n",
      "Epoch 230/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.3406 - acc: 0.9124 - val_loss: 0.9584 - val_acc: 0.7016\n",
      "Epoch 231/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.3260 - acc: 0.9326 - val_loss: 0.9932 - val_acc: 0.7137\n",
      "Epoch 232/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.3384 - acc: 0.9124 - val_loss: 0.9623 - val_acc: 0.7177\n",
      "Epoch 233/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.3373 - acc: 0.9191 - val_loss: 1.0175 - val_acc: 0.6935\n",
      "Epoch 234/600\n",
      "742/742 [==============================] - 0s 259us/step - loss: 0.3229 - acc: 0.9151 - val_loss: 0.9563 - val_acc: 0.7339\n",
      "Epoch 235/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.3528 - acc: 0.9070 - val_loss: 0.9594 - val_acc: 0.6815\n",
      "Epoch 236/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.3335 - acc: 0.9137 - val_loss: 0.9361 - val_acc: 0.7419\n",
      "Epoch 237/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.3268 - acc: 0.9245 - val_loss: 0.9975 - val_acc: 0.7298\n",
      "Epoch 238/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.3192 - acc: 0.9205 - val_loss: 0.9372 - val_acc: 0.7137\n",
      "Epoch 239/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.3176 - acc: 0.9232 - val_loss: 0.9491 - val_acc: 0.7218\n",
      "Epoch 240/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 249us/step - loss: 0.3264 - acc: 0.9191 - val_loss: 0.9803 - val_acc: 0.7016\n",
      "Epoch 241/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.3362 - acc: 0.9272 - val_loss: 0.9265 - val_acc: 0.7218\n",
      "Epoch 242/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.3419 - acc: 0.9070 - val_loss: 0.8953 - val_acc: 0.7218\n",
      "Epoch 243/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.3135 - acc: 0.9272 - val_loss: 0.9305 - val_acc: 0.7258\n",
      "Epoch 244/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.3337 - acc: 0.9259 - val_loss: 0.9349 - val_acc: 0.7258\n",
      "Epoch 245/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.3181 - acc: 0.9232 - val_loss: 0.9353 - val_acc: 0.7137\n",
      "Epoch 246/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.3296 - acc: 0.9218 - val_loss: 0.9026 - val_acc: 0.7258\n",
      "Epoch 247/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.3321 - acc: 0.9124 - val_loss: 0.9765 - val_acc: 0.7258\n",
      "Epoch 248/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.3206 - acc: 0.9218 - val_loss: 0.9278 - val_acc: 0.7258\n",
      "Epoch 249/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.3153 - acc: 0.9286 - val_loss: 0.9714 - val_acc: 0.7177\n",
      "Epoch 250/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.3239 - acc: 0.9151 - val_loss: 1.0010 - val_acc: 0.7097\n",
      "Epoch 251/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.3210 - acc: 0.9286 - val_loss: 0.9802 - val_acc: 0.7218\n",
      "Epoch 252/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.3031 - acc: 0.9272 - val_loss: 0.9753 - val_acc: 0.7056\n",
      "Epoch 253/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.3154 - acc: 0.9259 - val_loss: 0.9882 - val_acc: 0.7137\n",
      "Epoch 254/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.3304 - acc: 0.9137 - val_loss: 1.0258 - val_acc: 0.7137\n",
      "Epoch 255/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.3175 - acc: 0.9218 - val_loss: 1.0179 - val_acc: 0.7137\n",
      "Epoch 256/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.2940 - acc: 0.9353 - val_loss: 0.9234 - val_acc: 0.7137\n",
      "Epoch 257/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.3121 - acc: 0.9232 - val_loss: 0.9257 - val_acc: 0.7137\n",
      "Epoch 258/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.3160 - acc: 0.9191 - val_loss: 0.9384 - val_acc: 0.7218\n",
      "Epoch 259/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.2973 - acc: 0.9313 - val_loss: 0.9882 - val_acc: 0.6976\n",
      "Epoch 260/600\n",
      "742/742 [==============================] - 0s 261us/step - loss: 0.2969 - acc: 0.9299 - val_loss: 0.9662 - val_acc: 0.7016\n",
      "Epoch 261/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.3137 - acc: 0.9299 - val_loss: 0.9457 - val_acc: 0.7258\n",
      "Epoch 262/600\n",
      "742/742 [==============================] - 0s 218us/step - loss: 0.3144 - acc: 0.9232 - val_loss: 0.9250 - val_acc: 0.7298\n",
      "Epoch 263/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.2900 - acc: 0.9326 - val_loss: 0.9509 - val_acc: 0.7218\n",
      "Epoch 264/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.3018 - acc: 0.9353 - val_loss: 0.9966 - val_acc: 0.7056\n",
      "Epoch 265/600\n",
      "742/742 [==============================] - 0s 269us/step - loss: 0.3083 - acc: 0.9218 - val_loss: 0.9355 - val_acc: 0.7177\n",
      "Epoch 266/600\n",
      "742/742 [==============================] - 0s 266us/step - loss: 0.3135 - acc: 0.9299 - val_loss: 0.9093 - val_acc: 0.7661\n",
      "Epoch 267/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.2913 - acc: 0.9380 - val_loss: 1.0046 - val_acc: 0.7097\n",
      "Epoch 268/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.2959 - acc: 0.9326 - val_loss: 0.9836 - val_acc: 0.7056\n",
      "Epoch 269/600\n",
      "742/742 [==============================] - 0s 311us/step - loss: 0.2957 - acc: 0.9353 - val_loss: 0.9592 - val_acc: 0.6935\n",
      "Epoch 270/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.3018 - acc: 0.9340 - val_loss: 0.9667 - val_acc: 0.7056\n",
      "Epoch 271/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.3037 - acc: 0.9286 - val_loss: 0.9691 - val_acc: 0.6976\n",
      "Epoch 272/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.2869 - acc: 0.9326 - val_loss: 0.9385 - val_acc: 0.7419\n",
      "Epoch 273/600\n",
      "742/742 [==============================] - 0s 207us/step - loss: 0.2886 - acc: 0.9299 - val_loss: 0.8869 - val_acc: 0.7540\n",
      "Epoch 274/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.2927 - acc: 0.9340 - val_loss: 0.9526 - val_acc: 0.7379\n",
      "Epoch 275/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.2990 - acc: 0.9286 - val_loss: 0.9326 - val_acc: 0.7258\n",
      "Epoch 276/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.2836 - acc: 0.9380 - val_loss: 0.9808 - val_acc: 0.6976\n",
      "Epoch 277/600\n",
      "742/742 [==============================] - 0s 258us/step - loss: 0.2897 - acc: 0.9286 - val_loss: 0.9148 - val_acc: 0.7460\n",
      "Epoch 278/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.2828 - acc: 0.9434 - val_loss: 0.8882 - val_acc: 0.7218\n",
      "Epoch 279/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.3018 - acc: 0.9286 - val_loss: 0.9104 - val_acc: 0.7097\n",
      "Epoch 280/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.2843 - acc: 0.9353 - val_loss: 0.9814 - val_acc: 0.6935\n",
      "Epoch 281/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.2932 - acc: 0.9367 - val_loss: 0.9021 - val_acc: 0.7056\n",
      "Epoch 282/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.2874 - acc: 0.9299 - val_loss: 0.9605 - val_acc: 0.7177\n",
      "Epoch 283/600\n",
      "742/742 [==============================] - 0s 278us/step - loss: 0.2733 - acc: 0.9394 - val_loss: 0.8882 - val_acc: 0.7137\n",
      "Epoch 284/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.2772 - acc: 0.9353 - val_loss: 0.9473 - val_acc: 0.7379\n",
      "Epoch 285/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.2885 - acc: 0.9326 - val_loss: 0.9455 - val_acc: 0.7177\n",
      "Epoch 286/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.2706 - acc: 0.9420 - val_loss: 0.9512 - val_acc: 0.7137\n",
      "Epoch 287/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.2677 - acc: 0.9380 - val_loss: 0.9279 - val_acc: 0.7056\n",
      "Epoch 288/600\n",
      "742/742 [==============================] - 0s 255us/step - loss: 0.2811 - acc: 0.9434 - val_loss: 0.8908 - val_acc: 0.7460\n",
      "Epoch 289/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.2760 - acc: 0.9326 - val_loss: 0.9704 - val_acc: 0.7218\n",
      "Epoch 290/600\n",
      "742/742 [==============================] - 0s 224us/step - loss: 0.2770 - acc: 0.9326 - val_loss: 0.8743 - val_acc: 0.7379\n",
      "Epoch 291/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.2764 - acc: 0.9394 - val_loss: 1.0155 - val_acc: 0.6895\n",
      "Epoch 292/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.2761 - acc: 0.9313 - val_loss: 0.9492 - val_acc: 0.7379\n",
      "Epoch 293/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.2596 - acc: 0.9474 - val_loss: 0.9284 - val_acc: 0.7339\n",
      "Epoch 294/600\n",
      "742/742 [==============================] - 0s 259us/step - loss: 0.2730 - acc: 0.9353 - val_loss: 0.9866 - val_acc: 0.7218\n",
      "Epoch 295/600\n",
      "742/742 [==============================] - 0s 224us/step - loss: 0.2568 - acc: 0.9474 - val_loss: 0.9624 - val_acc: 0.7137\n",
      "Epoch 296/600\n",
      "742/742 [==============================] - 0s 216us/step - loss: 0.2782 - acc: 0.9286 - val_loss: 0.9651 - val_acc: 0.7137\n",
      "Epoch 297/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.2787 - acc: 0.9367 - val_loss: 0.8993 - val_acc: 0.7258\n",
      "Epoch 298/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.2704 - acc: 0.9420 - val_loss: 0.9091 - val_acc: 0.7298\n",
      "Epoch 299/600\n",
      "742/742 [==============================] - 0s 264us/step - loss: 0.2643 - acc: 0.9434 - val_loss: 0.9152 - val_acc: 0.7218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.2644 - acc: 0.9420 - val_loss: 0.9572 - val_acc: 0.6935\n",
      "Epoch 301/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.2728 - acc: 0.9380 - val_loss: 0.9613 - val_acc: 0.7258\n",
      "Epoch 302/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.2525 - acc: 0.9501 - val_loss: 0.9379 - val_acc: 0.7218\n",
      "Epoch 303/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.2722 - acc: 0.9353 - val_loss: 0.9220 - val_acc: 0.7137\n",
      "Epoch 304/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.2537 - acc: 0.9461 - val_loss: 0.8823 - val_acc: 0.7460\n",
      "Epoch 305/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.2636 - acc: 0.9420 - val_loss: 0.9427 - val_acc: 0.7137\n",
      "Epoch 306/600\n",
      "742/742 [==============================] - 0s 216us/step - loss: 0.2734 - acc: 0.9313 - val_loss: 0.9957 - val_acc: 0.7177\n",
      "Epoch 307/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.2578 - acc: 0.9461 - val_loss: 0.8939 - val_acc: 0.7339\n",
      "Epoch 308/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.2684 - acc: 0.9353 - val_loss: 0.9001 - val_acc: 0.7258\n",
      "Epoch 309/600\n",
      "742/742 [==============================] - 0s 270us/step - loss: 0.2534 - acc: 0.9461 - val_loss: 0.8907 - val_acc: 0.7540\n",
      "Epoch 310/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.2698 - acc: 0.9380 - val_loss: 0.9379 - val_acc: 0.7298\n",
      "Epoch 311/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.2532 - acc: 0.9474 - val_loss: 0.8514 - val_acc: 0.7339\n",
      "Epoch 312/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.2712 - acc: 0.9340 - val_loss: 0.8871 - val_acc: 0.7460\n",
      "Epoch 313/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.2463 - acc: 0.9488 - val_loss: 0.9418 - val_acc: 0.7258\n",
      "Epoch 314/600\n",
      "742/742 [==============================] - 0s 269us/step - loss: 0.2504 - acc: 0.9447 - val_loss: 0.8877 - val_acc: 0.7702\n",
      "Epoch 315/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.2499 - acc: 0.9501 - val_loss: 0.8524 - val_acc: 0.7581\n",
      "Epoch 316/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.2826 - acc: 0.9286 - val_loss: 0.9446 - val_acc: 0.7379\n",
      "Epoch 317/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.2497 - acc: 0.9447 - val_loss: 0.8840 - val_acc: 0.7339\n",
      "Epoch 318/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.2427 - acc: 0.9434 - val_loss: 0.9134 - val_acc: 0.7581\n",
      "Epoch 319/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.2444 - acc: 0.9488 - val_loss: 0.9281 - val_acc: 0.7097\n",
      "Epoch 320/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.2401 - acc: 0.9474 - val_loss: 0.9259 - val_acc: 0.7419\n",
      "Epoch 321/600\n",
      "742/742 [==============================] - 0s 204us/step - loss: 0.2651 - acc: 0.9447 - val_loss: 0.8814 - val_acc: 0.7339\n",
      "Epoch 322/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.2320 - acc: 0.9582 - val_loss: 0.8995 - val_acc: 0.7460\n",
      "Epoch 323/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.2360 - acc: 0.9474 - val_loss: 0.9430 - val_acc: 0.7218\n",
      "Epoch 324/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.2583 - acc: 0.9367 - val_loss: 0.8809 - val_acc: 0.7540\n",
      "Epoch 325/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.2370 - acc: 0.9528 - val_loss: 0.9149 - val_acc: 0.7097\n",
      "Epoch 326/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.2469 - acc: 0.9515 - val_loss: 0.9177 - val_acc: 0.7218\n",
      "Epoch 327/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.2542 - acc: 0.9380 - val_loss: 0.9770 - val_acc: 0.7218\n",
      "Epoch 328/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.2370 - acc: 0.9474 - val_loss: 0.8952 - val_acc: 0.7218\n",
      "Epoch 329/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.2452 - acc: 0.9474 - val_loss: 0.9254 - val_acc: 0.7419\n",
      "Epoch 330/600\n",
      "742/742 [==============================] - 0s 259us/step - loss: 0.2412 - acc: 0.9434 - val_loss: 0.9885 - val_acc: 0.7298\n",
      "Epoch 331/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.2481 - acc: 0.9542 - val_loss: 0.9000 - val_acc: 0.7500\n",
      "Epoch 332/600\n",
      "742/742 [==============================] - 0s 305us/step - loss: 0.2320 - acc: 0.9474 - val_loss: 1.0113 - val_acc: 0.7016\n",
      "Epoch 333/600\n",
      "742/742 [==============================] - 0s 320us/step - loss: 0.2442 - acc: 0.9434 - val_loss: 0.8608 - val_acc: 0.7339\n",
      "Epoch 334/600\n",
      "742/742 [==============================] - 0s 258us/step - loss: 0.2220 - acc: 0.9542 - val_loss: 0.9766 - val_acc: 0.6855\n",
      "Epoch 335/600\n",
      "742/742 [==============================] - 0s 260us/step - loss: 0.2484 - acc: 0.9367 - val_loss: 0.9156 - val_acc: 0.7379\n",
      "Epoch 336/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.2310 - acc: 0.9542 - val_loss: 0.9042 - val_acc: 0.7379\n",
      "Epoch 337/600\n",
      "742/742 [==============================] - 0s 266us/step - loss: 0.2285 - acc: 0.9461 - val_loss: 0.8790 - val_acc: 0.7540\n",
      "Epoch 338/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.2358 - acc: 0.9461 - val_loss: 0.9645 - val_acc: 0.7218\n",
      "Epoch 339/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.2530 - acc: 0.9461 - val_loss: 0.9409 - val_acc: 0.7137\n",
      "Epoch 340/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.2285 - acc: 0.9582 - val_loss: 0.9446 - val_acc: 0.7218\n",
      "Epoch 341/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.2205 - acc: 0.9528 - val_loss: 0.9037 - val_acc: 0.7218\n",
      "Epoch 342/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.2322 - acc: 0.9501 - val_loss: 0.9378 - val_acc: 0.7298\n",
      "Epoch 343/600\n",
      "742/742 [==============================] - 0s 271us/step - loss: 0.2230 - acc: 0.9407 - val_loss: 0.9934 - val_acc: 0.7137\n",
      "Epoch 344/600\n",
      "742/742 [==============================] - 0s 266us/step - loss: 0.2449 - acc: 0.9394 - val_loss: 0.9501 - val_acc: 0.7460\n",
      "Epoch 345/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.2309 - acc: 0.9515 - val_loss: 0.8789 - val_acc: 0.7581\n",
      "Epoch 346/600\n",
      "742/742 [==============================] - 0s 255us/step - loss: 0.2133 - acc: 0.9569 - val_loss: 1.0391 - val_acc: 0.6694\n",
      "Epoch 347/600\n",
      "742/742 [==============================] - 0s 219us/step - loss: 0.2244 - acc: 0.9501 - val_loss: 0.8635 - val_acc: 0.7581\n",
      "Epoch 348/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.2284 - acc: 0.9447 - val_loss: 0.9249 - val_acc: 0.7016\n",
      "Epoch 349/600\n",
      "742/742 [==============================] - 0s 294us/step - loss: 0.2200 - acc: 0.9555 - val_loss: 0.9781 - val_acc: 0.7298\n",
      "Epoch 350/600\n",
      "742/742 [==============================] - 0s 263us/step - loss: 0.2211 - acc: 0.9528 - val_loss: 0.9615 - val_acc: 0.7218\n",
      "Epoch 351/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.2247 - acc: 0.9501 - val_loss: 0.9272 - val_acc: 0.7540\n",
      "Epoch 352/600\n",
      "742/742 [==============================] - 0s 262us/step - loss: 0.2204 - acc: 0.9515 - val_loss: 1.0020 - val_acc: 0.7218\n",
      "Epoch 353/600\n",
      "742/742 [==============================] - 0s 264us/step - loss: 0.2258 - acc: 0.9555 - val_loss: 0.9959 - val_acc: 0.7298\n",
      "Epoch 354/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.2139 - acc: 0.9542 - val_loss: 0.8813 - val_acc: 0.7339\n",
      "Epoch 355/600\n",
      "742/742 [==============================] - 0s 268us/step - loss: 0.2133 - acc: 0.9636 - val_loss: 0.8944 - val_acc: 0.7419\n",
      "Epoch 356/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.2287 - acc: 0.9461 - val_loss: 0.9542 - val_acc: 0.7097\n",
      "Epoch 357/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.2351 - acc: 0.9434 - val_loss: 0.9522 - val_acc: 0.7137\n",
      "Epoch 358/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.2118 - acc: 0.9596 - val_loss: 0.8889 - val_acc: 0.7460\n",
      "Epoch 359/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.2050 - acc: 0.9623 - val_loss: 0.9653 - val_acc: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/600\n",
      "742/742 [==============================] - 0s 255us/step - loss: 0.2379 - acc: 0.9394 - val_loss: 0.9023 - val_acc: 0.7500\n",
      "Epoch 361/600\n",
      "742/742 [==============================] - 0s 274us/step - loss: 0.1969 - acc: 0.9650 - val_loss: 0.9007 - val_acc: 0.7339\n",
      "Epoch 362/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.2131 - acc: 0.9528 - val_loss: 0.9514 - val_acc: 0.7339\n",
      "Epoch 363/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.2153 - acc: 0.9434 - val_loss: 0.9488 - val_acc: 0.6935\n",
      "Epoch 364/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.2058 - acc: 0.9596 - val_loss: 0.9677 - val_acc: 0.7097\n",
      "Epoch 365/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.2222 - acc: 0.9447 - val_loss: 0.9199 - val_acc: 0.7298\n",
      "Epoch 366/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.2044 - acc: 0.9555 - val_loss: 0.9487 - val_acc: 0.7298\n",
      "Epoch 367/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.2241 - acc: 0.9474 - val_loss: 0.8471 - val_acc: 0.7581\n",
      "Epoch 368/600\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.2054 - acc: 0.953 - 0s 233us/step - loss: 0.2059 - acc: 0.9528 - val_loss: 0.8932 - val_acc: 0.7540\n",
      "Epoch 369/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.1919 - acc: 0.9677 - val_loss: 0.9467 - val_acc: 0.7339\n",
      "Epoch 370/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.2157 - acc: 0.9515 - val_loss: 0.9803 - val_acc: 0.7218\n",
      "Epoch 371/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.2070 - acc: 0.9596 - val_loss: 0.8992 - val_acc: 0.7258\n",
      "Epoch 372/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.2097 - acc: 0.9515 - val_loss: 0.9470 - val_acc: 0.7258\n",
      "Epoch 373/600\n",
      "742/742 [==============================] - 0s 225us/step - loss: 0.2027 - acc: 0.9501 - val_loss: 0.8958 - val_acc: 0.7540\n",
      "Epoch 374/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.2162 - acc: 0.9488 - val_loss: 0.9383 - val_acc: 0.7419\n",
      "Epoch 375/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.2103 - acc: 0.9555 - val_loss: 0.9687 - val_acc: 0.7298\n",
      "Epoch 376/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.2015 - acc: 0.9609 - val_loss: 0.8900 - val_acc: 0.7379\n",
      "Epoch 377/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.1964 - acc: 0.9542 - val_loss: 0.8776 - val_acc: 0.7419\n",
      "Epoch 378/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.1828 - acc: 0.9704 - val_loss: 0.9805 - val_acc: 0.7218\n",
      "Epoch 379/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.2108 - acc: 0.9569 - val_loss: 0.8543 - val_acc: 0.7379\n",
      "Epoch 380/600\n",
      "742/742 [==============================] - 0s 208us/step - loss: 0.1919 - acc: 0.9596 - val_loss: 0.9965 - val_acc: 0.7137\n",
      "Epoch 381/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.2000 - acc: 0.9555 - val_loss: 0.8866 - val_acc: 0.7540\n",
      "Epoch 382/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.2004 - acc: 0.9555 - val_loss: 0.9046 - val_acc: 0.7218\n",
      "Epoch 383/600\n",
      "742/742 [==============================] - 0s 260us/step - loss: 0.2147 - acc: 0.9542 - val_loss: 0.9996 - val_acc: 0.7339\n",
      "Epoch 384/600\n",
      "742/742 [==============================] - 0s 227us/step - loss: 0.1966 - acc: 0.9609 - val_loss: 0.9344 - val_acc: 0.7137\n",
      "Epoch 385/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.1871 - acc: 0.9677 - val_loss: 0.9191 - val_acc: 0.7298\n",
      "Epoch 386/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.2030 - acc: 0.9555 - val_loss: 0.8506 - val_acc: 0.7419\n",
      "Epoch 387/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.2003 - acc: 0.9650 - val_loss: 0.8967 - val_acc: 0.7419\n",
      "Epoch 388/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.1871 - acc: 0.9636 - val_loss: 0.9722 - val_acc: 0.7258\n",
      "Epoch 389/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.1972 - acc: 0.9596 - val_loss: 0.9156 - val_acc: 0.7298\n",
      "Epoch 390/600\n",
      "742/742 [==============================] - 0s 266us/step - loss: 0.1995 - acc: 0.9555 - val_loss: 0.9045 - val_acc: 0.7460\n",
      "Epoch 391/600\n",
      "742/742 [==============================] - 0s 260us/step - loss: 0.1912 - acc: 0.9569 - val_loss: 0.9594 - val_acc: 0.7298\n",
      "Epoch 392/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.1892 - acc: 0.9623 - val_loss: 0.9218 - val_acc: 0.7339\n",
      "Epoch 393/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.2059 - acc: 0.9542 - val_loss: 0.9910 - val_acc: 0.7258\n",
      "Epoch 394/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.1854 - acc: 0.9650 - val_loss: 0.9020 - val_acc: 0.7258\n",
      "Epoch 395/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.1903 - acc: 0.9555 - val_loss: 0.9005 - val_acc: 0.7460\n",
      "Epoch 396/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.1834 - acc: 0.9636 - val_loss: 0.9045 - val_acc: 0.7339\n",
      "Epoch 397/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.1871 - acc: 0.9623 - val_loss: 0.8655 - val_acc: 0.7621\n",
      "Epoch 398/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.1802 - acc: 0.9650 - val_loss: 0.8832 - val_acc: 0.7540\n",
      "Epoch 399/600\n",
      "742/742 [==============================] - 0s 271us/step - loss: 0.1774 - acc: 0.9717 - val_loss: 0.9928 - val_acc: 0.7419\n",
      "Epoch 400/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.1885 - acc: 0.9596 - val_loss: 0.9045 - val_acc: 0.7460\n",
      "Epoch 401/600\n",
      "742/742 [==============================] - 0s 266us/step - loss: 0.1898 - acc: 0.9623 - val_loss: 0.8908 - val_acc: 0.7218\n",
      "Epoch 402/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1817 - acc: 0.9650 - val_loss: 0.9082 - val_acc: 0.7419\n",
      "Epoch 403/600\n",
      "742/742 [==============================] - 0s 224us/step - loss: 0.1758 - acc: 0.9636 - val_loss: 0.9753 - val_acc: 0.6976\n",
      "Epoch 404/600\n",
      "742/742 [==============================] - 0s 268us/step - loss: 0.1918 - acc: 0.9596 - val_loss: 0.8431 - val_acc: 0.7419\n",
      "Epoch 405/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.1728 - acc: 0.9609 - val_loss: 0.8541 - val_acc: 0.7661\n",
      "Epoch 406/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.1841 - acc: 0.9677 - val_loss: 0.9187 - val_acc: 0.7298\n",
      "Epoch 407/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.1670 - acc: 0.9677 - val_loss: 0.9194 - val_acc: 0.7419\n",
      "Epoch 408/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1898 - acc: 0.9609 - val_loss: 0.9089 - val_acc: 0.7258\n",
      "Epoch 409/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.1912 - acc: 0.9650 - val_loss: 0.8782 - val_acc: 0.7379\n",
      "Epoch 410/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.1805 - acc: 0.9636 - val_loss: 0.8679 - val_acc: 0.7500\n",
      "Epoch 411/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.1765 - acc: 0.9677 - val_loss: 0.9847 - val_acc: 0.7419\n",
      "Epoch 412/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.1745 - acc: 0.9609 - val_loss: 0.9599 - val_acc: 0.7419\n",
      "Epoch 413/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.1757 - acc: 0.9636 - val_loss: 0.9633 - val_acc: 0.7419\n",
      "Epoch 414/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.1892 - acc: 0.9528 - val_loss: 0.9082 - val_acc: 0.7581\n",
      "Epoch 415/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.1731 - acc: 0.9650 - val_loss: 0.9232 - val_acc: 0.7379\n",
      "Epoch 416/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1768 - acc: 0.9677 - val_loss: 0.9425 - val_acc: 0.7298\n",
      "Epoch 417/600\n",
      "742/742 [==============================] - 0s 264us/step - loss: 0.1762 - acc: 0.9542 - val_loss: 0.8630 - val_acc: 0.7581\n",
      "Epoch 418/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.1800 - acc: 0.9650 - val_loss: 0.9786 - val_acc: 0.7339\n",
      "Epoch 419/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 224us/step - loss: 0.1703 - acc: 0.9650 - val_loss: 0.9611 - val_acc: 0.7258\n",
      "Epoch 420/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.1759 - acc: 0.9623 - val_loss: 0.9895 - val_acc: 0.7056\n",
      "Epoch 421/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.1814 - acc: 0.9555 - val_loss: 0.9361 - val_acc: 0.7419\n",
      "Epoch 422/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.1733 - acc: 0.9650 - val_loss: 0.9003 - val_acc: 0.7258\n",
      "Epoch 423/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.1562 - acc: 0.9757 - val_loss: 0.8852 - val_acc: 0.7540\n",
      "Epoch 424/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1646 - acc: 0.9744 - val_loss: 0.8385 - val_acc: 0.7581\n",
      "Epoch 425/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1827 - acc: 0.9596 - val_loss: 0.9259 - val_acc: 0.7339\n",
      "Epoch 426/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.1682 - acc: 0.9771 - val_loss: 0.8839 - val_acc: 0.7339\n",
      "Epoch 427/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.1645 - acc: 0.9690 - val_loss: 0.9312 - val_acc: 0.7460\n",
      "Epoch 428/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.1689 - acc: 0.9663 - val_loss: 0.9029 - val_acc: 0.7621\n",
      "Epoch 429/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.1567 - acc: 0.9690 - val_loss: 0.9423 - val_acc: 0.7298\n",
      "Epoch 430/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.1808 - acc: 0.9582 - val_loss: 0.9101 - val_acc: 0.7379\n",
      "Epoch 431/600\n",
      "742/742 [==============================] - 0s 225us/step - loss: 0.1673 - acc: 0.9636 - val_loss: 0.9352 - val_acc: 0.7218\n",
      "Epoch 432/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.1638 - acc: 0.9636 - val_loss: 0.9354 - val_acc: 0.7460\n",
      "Epoch 433/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.1600 - acc: 0.9704 - val_loss: 0.8435 - val_acc: 0.7460\n",
      "Epoch 434/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.1572 - acc: 0.9663 - val_loss: 0.8968 - val_acc: 0.7540\n",
      "Epoch 435/600\n",
      "742/742 [==============================] - 0s 215us/step - loss: 0.1803 - acc: 0.9623 - val_loss: 0.9468 - val_acc: 0.7540\n",
      "Epoch 436/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.1694 - acc: 0.9663 - val_loss: 0.8953 - val_acc: 0.7419\n",
      "Epoch 437/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.1430 - acc: 0.9798 - val_loss: 0.9729 - val_acc: 0.7097\n",
      "Epoch 438/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.1634 - acc: 0.9690 - val_loss: 0.9706 - val_acc: 0.7298\n",
      "Epoch 439/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.1625 - acc: 0.9663 - val_loss: 0.9418 - val_acc: 0.7540\n",
      "Epoch 440/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.1684 - acc: 0.9582 - val_loss: 0.9656 - val_acc: 0.7500\n",
      "Epoch 441/600\n",
      "742/742 [==============================] - 0s 219us/step - loss: 0.1499 - acc: 0.9757 - val_loss: 0.8831 - val_acc: 0.7742\n",
      "Epoch 442/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.1541 - acc: 0.9771 - val_loss: 0.8605 - val_acc: 0.7661\n",
      "Epoch 443/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.1755 - acc: 0.9623 - val_loss: 0.8827 - val_acc: 0.7500\n",
      "Epoch 444/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.1443 - acc: 0.9744 - val_loss: 0.8825 - val_acc: 0.7298\n",
      "Epoch 445/600\n",
      "742/742 [==============================] - 0s 236us/step - loss: 0.1499 - acc: 0.9744 - val_loss: 0.9538 - val_acc: 0.7177\n",
      "Epoch 446/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.1502 - acc: 0.9771 - val_loss: 0.9307 - val_acc: 0.7379\n",
      "Epoch 447/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1571 - acc: 0.9650 - val_loss: 0.9347 - val_acc: 0.7379\n",
      "Epoch 448/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.1506 - acc: 0.9690 - val_loss: 0.8877 - val_acc: 0.7621\n",
      "Epoch 449/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.1575 - acc: 0.9704 - val_loss: 0.9364 - val_acc: 0.7379\n",
      "Epoch 450/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.1452 - acc: 0.9771 - val_loss: 0.9338 - val_acc: 0.7460\n",
      "Epoch 451/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.1458 - acc: 0.9798 - val_loss: 0.8585 - val_acc: 0.7258\n",
      "Epoch 452/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.1503 - acc: 0.9663 - val_loss: 0.8829 - val_acc: 0.7540\n",
      "Epoch 453/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.1783 - acc: 0.9609 - val_loss: 0.9383 - val_acc: 0.7339\n",
      "Epoch 454/600\n",
      "742/742 [==============================] - 0s 225us/step - loss: 0.1673 - acc: 0.9609 - val_loss: 0.8919 - val_acc: 0.7581\n",
      "Epoch 455/600\n",
      "742/742 [==============================] - 0s 218us/step - loss: 0.1493 - acc: 0.9663 - val_loss: 0.9095 - val_acc: 0.7419\n",
      "Epoch 456/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.1425 - acc: 0.9757 - val_loss: 0.9338 - val_acc: 0.7339\n",
      "Epoch 457/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.1444 - acc: 0.9771 - val_loss: 0.9351 - val_acc: 0.7460\n",
      "Epoch 458/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.1394 - acc: 0.9784 - val_loss: 0.9960 - val_acc: 0.7460\n",
      "Epoch 459/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.1426 - acc: 0.9717 - val_loss: 0.9028 - val_acc: 0.7500\n",
      "Epoch 460/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.1517 - acc: 0.9730 - val_loss: 0.9524 - val_acc: 0.7339\n",
      "Epoch 461/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.1379 - acc: 0.9757 - val_loss: 1.0607 - val_acc: 0.7097\n",
      "Epoch 462/600\n",
      "742/742 [==============================] - 0s 266us/step - loss: 0.1493 - acc: 0.9730 - val_loss: 0.9458 - val_acc: 0.7097\n",
      "Epoch 463/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.1510 - acc: 0.9690 - val_loss: 0.9215 - val_acc: 0.7500\n",
      "Epoch 464/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.1441 - acc: 0.9757 - val_loss: 0.9378 - val_acc: 0.7581\n",
      "Epoch 465/600\n",
      "742/742 [==============================] - 0s 219us/step - loss: 0.1372 - acc: 0.9784 - val_loss: 0.9718 - val_acc: 0.7177\n",
      "Epoch 466/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.1525 - acc: 0.9717 - val_loss: 0.9685 - val_acc: 0.7339\n",
      "Epoch 467/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.1364 - acc: 0.9771 - val_loss: 0.9043 - val_acc: 0.7339\n",
      "Epoch 468/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.1528 - acc: 0.9690 - val_loss: 0.9164 - val_acc: 0.7460\n",
      "Epoch 469/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1363 - acc: 0.9784 - val_loss: 0.8912 - val_acc: 0.7258\n",
      "Epoch 470/600\n",
      "742/742 [==============================] - 0s 227us/step - loss: 0.1480 - acc: 0.9717 - val_loss: 0.8987 - val_acc: 0.7419\n",
      "Epoch 471/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.1461 - acc: 0.9663 - val_loss: 0.9219 - val_acc: 0.7298\n",
      "Epoch 472/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.1424 - acc: 0.9744 - val_loss: 0.8291 - val_acc: 0.7581\n",
      "Epoch 473/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.1379 - acc: 0.9825 - val_loss: 0.9735 - val_acc: 0.7460\n",
      "Epoch 474/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.1526 - acc: 0.9663 - val_loss: 0.9204 - val_acc: 0.7661\n",
      "Epoch 475/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.1461 - acc: 0.9730 - val_loss: 0.9050 - val_acc: 0.7500\n",
      "Epoch 476/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1286 - acc: 0.9798 - val_loss: 0.9046 - val_acc: 0.7339\n",
      "Epoch 477/600\n",
      "742/742 [==============================] - 0s 217us/step - loss: 0.1341 - acc: 0.9744 - val_loss: 0.9541 - val_acc: 0.7379\n",
      "Epoch 478/600\n",
      "742/742 [==============================] - 0s 227us/step - loss: 0.1421 - acc: 0.9717 - val_loss: 0.9172 - val_acc: 0.7339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479/600\n",
      "742/742 [==============================] - 0s 239us/step - loss: 0.1332 - acc: 0.9771 - val_loss: 0.8609 - val_acc: 0.7581\n",
      "Epoch 480/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.1274 - acc: 0.9811 - val_loss: 0.9592 - val_acc: 0.7460\n",
      "Epoch 481/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.1566 - acc: 0.9704 - val_loss: 0.9378 - val_acc: 0.7500\n",
      "Epoch 482/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.1263 - acc: 0.9784 - val_loss: 0.9038 - val_acc: 0.7460\n",
      "Epoch 483/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.1247 - acc: 0.9784 - val_loss: 0.9079 - val_acc: 0.7581\n",
      "Epoch 484/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.1515 - acc: 0.9717 - val_loss: 0.8894 - val_acc: 0.7500\n",
      "Epoch 485/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.1276 - acc: 0.9838 - val_loss: 0.9145 - val_acc: 0.7460\n",
      "Epoch 486/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.1308 - acc: 0.9811 - val_loss: 0.8965 - val_acc: 0.7540\n",
      "Epoch 487/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.1321 - acc: 0.9744 - val_loss: 0.8451 - val_acc: 0.7419\n",
      "Epoch 488/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.1368 - acc: 0.9717 - val_loss: 0.9372 - val_acc: 0.7218\n",
      "Epoch 489/600\n",
      "742/742 [==============================] - 0s 214us/step - loss: 0.1280 - acc: 0.9825 - val_loss: 0.9466 - val_acc: 0.7540\n",
      "Epoch 490/600\n",
      "742/742 [==============================] - 0s 258us/step - loss: 0.1346 - acc: 0.9784 - val_loss: 0.8761 - val_acc: 0.7540\n",
      "Epoch 491/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.1220 - acc: 0.9865 - val_loss: 0.9100 - val_acc: 0.7339\n",
      "Epoch 492/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.1202 - acc: 0.9771 - val_loss: 0.9252 - val_acc: 0.7218\n",
      "Epoch 493/600\n",
      "742/742 [==============================] - 0s 268us/step - loss: 0.1278 - acc: 0.9771 - val_loss: 0.8664 - val_acc: 0.7419\n",
      "Epoch 494/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.1383 - acc: 0.9704 - val_loss: 0.9061 - val_acc: 0.7379\n",
      "Epoch 495/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.1274 - acc: 0.9757 - val_loss: 0.8846 - val_acc: 0.7258\n",
      "Epoch 496/600\n",
      "742/742 [==============================] - 0s 225us/step - loss: 0.1176 - acc: 0.9798 - val_loss: 0.9008 - val_acc: 0.7621\n",
      "Epoch 497/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.1284 - acc: 0.9838 - val_loss: 0.9048 - val_acc: 0.7621\n",
      "Epoch 498/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.1299 - acc: 0.9771 - val_loss: 1.0578 - val_acc: 0.7218\n",
      "Epoch 499/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.1255 - acc: 0.9771 - val_loss: 0.8844 - val_acc: 0.7581\n",
      "Epoch 500/600\n",
      "742/742 [==============================] - 0s 259us/step - loss: 0.1202 - acc: 0.9825 - val_loss: 0.8918 - val_acc: 0.7661\n",
      "Epoch 501/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.1277 - acc: 0.9771 - val_loss: 0.9115 - val_acc: 0.7460\n",
      "Epoch 502/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.1249 - acc: 0.9811 - val_loss: 0.9462 - val_acc: 0.7258\n",
      "Epoch 503/600\n",
      "742/742 [==============================] - 0s 263us/step - loss: 0.1344 - acc: 0.9730 - val_loss: 0.8697 - val_acc: 0.7379\n",
      "Epoch 504/600\n",
      "742/742 [==============================] - 0s 240us/step - loss: 0.1141 - acc: 0.9811 - val_loss: 0.9088 - val_acc: 0.7419\n",
      "Epoch 505/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.1227 - acc: 0.9784 - val_loss: 0.8956 - val_acc: 0.7379\n",
      "Epoch 506/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.1179 - acc: 0.9798 - val_loss: 0.9123 - val_acc: 0.7500\n",
      "Epoch 507/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.1265 - acc: 0.9811 - val_loss: 1.0118 - val_acc: 0.7339\n",
      "Epoch 508/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.1330 - acc: 0.9744 - val_loss: 0.9841 - val_acc: 0.7177\n",
      "Epoch 509/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.1240 - acc: 0.9771 - val_loss: 0.9323 - val_acc: 0.7500\n",
      "Epoch 510/600\n",
      "742/742 [==============================] - 0s 251us/step - loss: 0.1088 - acc: 0.9852 - val_loss: 0.8883 - val_acc: 0.7581\n",
      "Epoch 511/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.1156 - acc: 0.9798 - val_loss: 0.8685 - val_acc: 0.7742\n",
      "Epoch 512/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.1155 - acc: 0.9811 - val_loss: 0.9971 - val_acc: 0.7097\n",
      "Epoch 513/600\n",
      "742/742 [==============================] - 0s 224us/step - loss: 0.1383 - acc: 0.9757 - val_loss: 0.9682 - val_acc: 0.7177\n",
      "Epoch 514/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.1180 - acc: 0.9879 - val_loss: 0.8984 - val_acc: 0.7661\n",
      "Epoch 515/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.1104 - acc: 0.9852 - val_loss: 0.9257 - val_acc: 0.7419\n",
      "Epoch 516/600\n",
      "742/742 [==============================] - 0s 258us/step - loss: 0.1216 - acc: 0.9852 - val_loss: 0.8934 - val_acc: 0.7581\n",
      "Epoch 517/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.1208 - acc: 0.9784 - val_loss: 0.9456 - val_acc: 0.7339\n",
      "Epoch 518/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.1135 - acc: 0.9825 - val_loss: 0.9158 - val_acc: 0.7500\n",
      "Epoch 519/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.1197 - acc: 0.9798 - val_loss: 0.9268 - val_acc: 0.7500\n",
      "Epoch 520/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.1159 - acc: 0.9825 - val_loss: 0.8919 - val_acc: 0.7540\n",
      "Epoch 521/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.1147 - acc: 0.9825 - val_loss: 0.9885 - val_acc: 0.7339\n",
      "Epoch 522/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.1124 - acc: 0.9852 - val_loss: 0.8521 - val_acc: 0.7702\n",
      "Epoch 523/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1242 - acc: 0.9811 - val_loss: 0.9271 - val_acc: 0.7379\n",
      "Epoch 524/600\n",
      "742/742 [==============================] - 0s 225us/step - loss: 0.1131 - acc: 0.9811 - val_loss: 0.9182 - val_acc: 0.7621\n",
      "Epoch 525/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.1023 - acc: 0.9879 - val_loss: 1.0182 - val_acc: 0.7137\n",
      "Epoch 526/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.1136 - acc: 0.9825 - val_loss: 0.8885 - val_acc: 0.7581\n",
      "Epoch 527/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.1103 - acc: 0.9852 - val_loss: 0.9647 - val_acc: 0.7218\n",
      "Epoch 528/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.1128 - acc: 0.9811 - val_loss: 0.8901 - val_acc: 0.7621\n",
      "Epoch 529/600\n",
      "742/742 [==============================] - 0s 223us/step - loss: 0.1169 - acc: 0.9771 - val_loss: 0.9593 - val_acc: 0.7419\n",
      "Epoch 530/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.1255 - acc: 0.9757 - val_loss: 0.8783 - val_acc: 0.7298\n",
      "Epoch 531/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.1040 - acc: 0.9852 - val_loss: 0.9585 - val_acc: 0.7379\n",
      "Epoch 532/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.1110 - acc: 0.9838 - val_loss: 0.9799 - val_acc: 0.7218\n",
      "Epoch 533/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.1060 - acc: 0.9811 - val_loss: 0.8890 - val_acc: 0.7379\n",
      "Epoch 534/600\n",
      "742/742 [==============================] - 0s 243us/step - loss: 0.1030 - acc: 0.9811 - val_loss: 0.8661 - val_acc: 0.7500\n",
      "Epoch 535/600\n",
      "742/742 [==============================] - 0s 206us/step - loss: 0.1014 - acc: 0.9879 - val_loss: 0.9902 - val_acc: 0.7298\n",
      "Epoch 536/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.1167 - acc: 0.9798 - val_loss: 0.9624 - val_acc: 0.7419\n",
      "Epoch 537/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.1129 - acc: 0.9825 - val_loss: 0.8728 - val_acc: 0.7540\n",
      "Epoch 538/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.1015 - acc: 0.9865 - val_loss: 0.9259 - val_acc: 0.7379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 539/600\n",
      "742/742 [==============================] - 0s 263us/step - loss: 0.0957 - acc: 0.9919 - val_loss: 0.8458 - val_acc: 0.7540\n",
      "Epoch 540/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.1050 - acc: 0.9825 - val_loss: 0.9628 - val_acc: 0.7540\n",
      "Epoch 541/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.1065 - acc: 0.9865 - val_loss: 0.8827 - val_acc: 0.7500\n",
      "Epoch 542/600\n",
      "742/742 [==============================] - 0s 215us/step - loss: 0.1180 - acc: 0.9825 - val_loss: 0.9014 - val_acc: 0.7702\n",
      "Epoch 543/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.1086 - acc: 0.9825 - val_loss: 0.9894 - val_acc: 0.7056\n",
      "Epoch 544/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.0978 - acc: 0.9906 - val_loss: 1.0124 - val_acc: 0.7258\n",
      "Epoch 545/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.0976 - acc: 0.9879 - val_loss: 0.9610 - val_acc: 0.7137\n",
      "Epoch 546/600\n",
      "742/742 [==============================] - 0s 214us/step - loss: 0.1077 - acc: 0.9852 - val_loss: 1.0475 - val_acc: 0.7258\n",
      "Epoch 547/600\n",
      "742/742 [==============================] - 0s 271us/step - loss: 0.1133 - acc: 0.9825 - val_loss: 0.8795 - val_acc: 0.7702\n",
      "Epoch 548/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.1028 - acc: 0.9865 - val_loss: 0.9046 - val_acc: 0.7540\n",
      "Epoch 549/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.0953 - acc: 0.9865 - val_loss: 0.9560 - val_acc: 0.7540\n",
      "Epoch 550/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.1081 - acc: 0.9825 - val_loss: 0.8857 - val_acc: 0.7621\n",
      "Epoch 551/600\n",
      "742/742 [==============================] - 0s 262us/step - loss: 0.0907 - acc: 0.9933 - val_loss: 0.8908 - val_acc: 0.7702\n",
      "Epoch 552/600\n",
      "742/742 [==============================] - 0s 212us/step - loss: 0.1027 - acc: 0.9865 - val_loss: 0.9192 - val_acc: 0.7581\n",
      "Epoch 553/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.1009 - acc: 0.9879 - val_loss: 0.8777 - val_acc: 0.7621\n",
      "Epoch 554/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.0988 - acc: 0.9838 - val_loss: 0.9879 - val_acc: 0.7621\n",
      "Epoch 555/600\n",
      "742/742 [==============================] - 0s 232us/step - loss: 0.1281 - acc: 0.9784 - val_loss: 0.8564 - val_acc: 0.7621\n",
      "Epoch 556/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.0985 - acc: 0.9865 - val_loss: 0.9313 - val_acc: 0.7379\n",
      "Epoch 557/600\n",
      "742/742 [==============================] - 0s 244us/step - loss: 0.0988 - acc: 0.9865 - val_loss: 0.9268 - val_acc: 0.7379\n",
      "Epoch 558/600\n",
      "742/742 [==============================] - 0s 237us/step - loss: 0.0918 - acc: 0.9906 - val_loss: 0.8581 - val_acc: 0.7581\n",
      "Epoch 559/600\n",
      "742/742 [==============================] - 0s 257us/step - loss: 0.0884 - acc: 0.9906 - val_loss: 0.9759 - val_acc: 0.7621\n",
      "Epoch 560/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.1049 - acc: 0.9852 - val_loss: 0.9822 - val_acc: 0.7379\n",
      "Epoch 561/600\n",
      "742/742 [==============================] - 0s 259us/step - loss: 0.0983 - acc: 0.9852 - val_loss: 0.9561 - val_acc: 0.7540\n",
      "Epoch 562/600\n",
      "742/742 [==============================] - 0s 250us/step - loss: 0.1031 - acc: 0.9879 - val_loss: 0.9113 - val_acc: 0.7702\n",
      "Epoch 563/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.0929 - acc: 0.9879 - val_loss: 0.9655 - val_acc: 0.7500\n",
      "Epoch 564/600\n",
      "742/742 [==============================] - 0s 222us/step - loss: 0.1035 - acc: 0.9865 - val_loss: 0.9009 - val_acc: 0.7500\n",
      "Epoch 565/600\n",
      "742/742 [==============================] - 0s 231us/step - loss: 0.0952 - acc: 0.9879 - val_loss: 0.8840 - val_acc: 0.7540\n",
      "Epoch 566/600\n",
      "742/742 [==============================] - 0s 230us/step - loss: 0.0968 - acc: 0.9852 - val_loss: 0.9281 - val_acc: 0.7621\n",
      "Epoch 567/600\n",
      "742/742 [==============================] - 0s 225us/step - loss: 0.0893 - acc: 0.9906 - val_loss: 0.9414 - val_acc: 0.7540\n",
      "Epoch 568/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.0928 - acc: 0.9852 - val_loss: 0.9026 - val_acc: 0.7621\n",
      "Epoch 569/600\n",
      "742/742 [==============================] - 0s 263us/step - loss: 0.1031 - acc: 0.9771 - val_loss: 0.9001 - val_acc: 0.7540\n",
      "Epoch 570/600\n",
      "742/742 [==============================] - 0s 202us/step - loss: 0.0937 - acc: 0.9865 - val_loss: 0.9865 - val_acc: 0.7581\n",
      "Epoch 571/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.0940 - acc: 0.9865 - val_loss: 0.8957 - val_acc: 0.7621\n",
      "Epoch 572/600\n",
      "742/742 [==============================] - 0s 238us/step - loss: 0.0969 - acc: 0.9825 - val_loss: 0.9127 - val_acc: 0.7379\n",
      "Epoch 573/600\n",
      "742/742 [==============================] - 0s 245us/step - loss: 0.0948 - acc: 0.9906 - val_loss: 0.9625 - val_acc: 0.7621\n",
      "Epoch 574/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.0874 - acc: 0.9906 - val_loss: 0.9460 - val_acc: 0.7621\n",
      "Epoch 575/600\n",
      "742/742 [==============================] - 0s 209us/step - loss: 0.0907 - acc: 0.9879 - val_loss: 0.9449 - val_acc: 0.7661\n",
      "Epoch 576/600\n",
      "742/742 [==============================] - 0s 261us/step - loss: 0.0911 - acc: 0.9865 - val_loss: 0.8887 - val_acc: 0.7540\n",
      "Epoch 577/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.0917 - acc: 0.9879 - val_loss: 0.8629 - val_acc: 0.7621\n",
      "Epoch 578/600\n",
      "742/742 [==============================] - 0s 228us/step - loss: 0.1209 - acc: 0.9744 - val_loss: 0.8830 - val_acc: 0.7581\n",
      "Epoch 579/600\n",
      "742/742 [==============================] - 0s 247us/step - loss: 0.0761 - acc: 0.9919 - val_loss: 0.9312 - val_acc: 0.7419\n",
      "Epoch 580/600\n",
      "742/742 [==============================] - 0s 204us/step - loss: 0.0812 - acc: 0.9933 - val_loss: 0.9376 - val_acc: 0.7339\n",
      "Epoch 581/600\n",
      "742/742 [==============================] - 0s 220us/step - loss: 0.0951 - acc: 0.9825 - val_loss: 0.9136 - val_acc: 0.7500\n",
      "Epoch 582/600\n",
      "742/742 [==============================] - 0s 248us/step - loss: 0.1026 - acc: 0.9892 - val_loss: 1.0346 - val_acc: 0.7218\n",
      "Epoch 583/600\n",
      "742/742 [==============================] - 0s 218us/step - loss: 0.0911 - acc: 0.9906 - val_loss: 0.8794 - val_acc: 0.7581\n",
      "Epoch 584/600\n",
      "742/742 [==============================] - 0s 235us/step - loss: 0.0833 - acc: 0.9906 - val_loss: 0.8433 - val_acc: 0.7621\n",
      "Epoch 585/600\n",
      "742/742 [==============================] - 0s 212us/step - loss: 0.0759 - acc: 0.9933 - val_loss: 0.9118 - val_acc: 0.7581\n",
      "Epoch 586/600\n",
      "742/742 [==============================] - 0s 252us/step - loss: 0.0877 - acc: 0.9879 - val_loss: 0.9527 - val_acc: 0.7460\n",
      "Epoch 587/600\n",
      "742/742 [==============================] - 0s 229us/step - loss: 0.0795 - acc: 0.9960 - val_loss: 0.9405 - val_acc: 0.7661\n",
      "Epoch 588/600\n",
      "742/742 [==============================] - 0s 246us/step - loss: 0.1055 - acc: 0.9879 - val_loss: 0.9485 - val_acc: 0.7661\n",
      "Epoch 589/600\n",
      "742/742 [==============================] - 0s 249us/step - loss: 0.0961 - acc: 0.9771 - val_loss: 0.8708 - val_acc: 0.7621\n",
      "Epoch 590/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.0724 - acc: 0.9933 - val_loss: 0.9173 - val_acc: 0.7379\n",
      "Epoch 591/600\n",
      "742/742 [==============================] - 0s 253us/step - loss: 0.0845 - acc: 0.9906 - val_loss: 0.8851 - val_acc: 0.7540\n",
      "Epoch 592/600\n",
      "742/742 [==============================] - 0s 242us/step - loss: 0.0827 - acc: 0.9892 - val_loss: 0.9319 - val_acc: 0.7540\n",
      "Epoch 593/600\n",
      "742/742 [==============================] - 0s 254us/step - loss: 0.1008 - acc: 0.9811 - val_loss: 0.9457 - val_acc: 0.7379\n",
      "Epoch 594/600\n",
      "742/742 [==============================] - 0s 221us/step - loss: 0.0937 - acc: 0.9852 - val_loss: 0.8808 - val_acc: 0.7621\n",
      "Epoch 595/600\n",
      "742/742 [==============================] - 0s 256us/step - loss: 0.0924 - acc: 0.9879 - val_loss: 0.9076 - val_acc: 0.7581\n",
      "Epoch 596/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.0757 - acc: 0.9906 - val_loss: 0.9389 - val_acc: 0.7460\n",
      "Epoch 597/600\n",
      "742/742 [==============================] - 0s 234us/step - loss: 0.0809 - acc: 0.9919 - val_loss: 0.8805 - val_acc: 0.7702\n",
      "Epoch 598/600\n",
      "742/742 [==============================] - 0s 241us/step - loss: 0.0797 - acc: 0.9906 - val_loss: 0.9157 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 599/600\n",
      "742/742 [==============================] - 0s 226us/step - loss: 0.0812 - acc: 0.9933 - val_loss: 0.9257 - val_acc: 0.7621\n",
      "Epoch 600/600\n",
      "742/742 [==============================] - 0s 233us/step - loss: 0.0932 - acc: 0.9865 - val_loss: 0.9484 - val_acc: 0.7661\n"
     ]
    }
   ],
   "source": [
    "# 因为有99种的分类，epoch必要要多：\n",
    "history = model.fit( train_x, train_y, epochs=600, batch_size=128, validation_data=(test_x, test_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28282051b88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history.get('acc'), label='acc')\n",
    "plt.plot(history.epoch, history.history.get('val_acc'), label='val_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的图，可以说明下面一些东西：\n",
    "- 首先，它的目标softmax多分类，划分的类别99种实在太多！现在在测试数据上的精度能到70+已经不简单了！\n",
    "- 由图可知：神经网络对“训练数据”的拟合精度还没到100%，也就是现在的网络的“精度上限”还有待进一步提高；—— 把网络加深；\n",
    "- 由图可知，训练精度和测试精度之间的鸿沟很大，且似乎没有缩小鸿沟的趋势。因为当前网络“过拟合”严重； —— 加dropout；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络优化1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
