{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里所使用的数据增强和前面所用到完全一样，就是对输入的数据进行一定的数学变换，将可提取的特征增多。\n",
    "\n",
    "缺点：既然“每轮”输入的数据不一样了，那就不能用“进阶”中的方法 —— 让卷积基只提取特征一次。因为现在每轮的图都不一样了，也就是每轮训练都会有新的特征要提取。—— 所以卷积基不能只用一次，要放入模型之中（权重参数还是定死的，因为卷积基还是只负责特征的提取）。—— 这样就和26中“基础”部分的程序基本一样了。\n",
    "\n",
    "缺点：训练时间会很长！因为大型网络的卷积基要参与计算了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  数据处理部分：加入数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil    # os和shutil用来处理文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据太多了，现在专门创建一个文件夹来存储一部分要用的训练集 + 测试集\n",
    "base_dir = 'E:/Python_code/keras_total/日月光华-keras课程资料/dc/try'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "train_dir_dog = os.path.join(train_dir , 'dog')\n",
    "train_dir_cat = os.path.join(train_dir , 'cat')\n",
    "\n",
    "test_dir = os.path.join(base_dir , 'test')\n",
    "test_dir_dog = os.path.join(test_dir , 'dog')\n",
    "test_dir_cat = os.path.join(test_dir , 'cat')\n",
    "\n",
    "dc_dir = 'E:/Python_code/keras_total/日月光华-keras课程资料/dc/train' # 原始数据所在路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集猫狗各1000张，测试集猫狗各500张。\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(train_dir_dog)\n",
    "    os.mkdir(train_dir_cat)\n",
    "    os.mkdir(test_dir)\n",
    "    os.mkdir(test_dir_dog)\n",
    "    os.mkdir(test_dir_cat)\n",
    "\n",
    "    fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "    for fname in fnames:\n",
    "        s = os.path.join(dc_dir, fname) \n",
    "        d = os.path.join(train_dir_cat, fname)\n",
    "        shutil.copyfile(s, d)   #  把s拷贝到d \n",
    "\n",
    "    fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "    for fname in fnames:\n",
    "        s = os.path.join(dc_dir, fname)\n",
    "        d = os.path.join(test_dir_cat, fname)\n",
    "        shutil.copyfile(s, d)\n",
    "\n",
    "    fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "    for fname in fnames:\n",
    "        s = os.path.join(dc_dir, fname)\n",
    "        d = os.path.join(train_dir_dog, fname)\n",
    "        shutil.copyfile(s, d)\n",
    "\n",
    "    fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "    for fname in fnames:\n",
    "        s = os.path.join(dc_dir, fname)\n",
    "        d = os.path.join(test_dir_dog, fname)\n",
    "        shutil.copyfile(s, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入数据增强： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 创建图片的迭代器：在里面加入数据增强的参数\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 40,\n",
    "    width_shift_range = 0.2, \n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2, \n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "# 注意：测试数据不能增强，因为增强了就不是原图了！\n",
    "test_datagen = ImageDataGenerator(1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 正式创建图片的生成器：train\n",
    "train_generator = train_datagen.flow_from_directory( train_dir,                  # 待读取文件的目录\n",
    "                                                     target_size = (200,200),    # 图片的统一大小 \n",
    "                                                     batch_size = 20,            # 每次读入20张\n",
    "                                                     class_mode = 'binary'       # 该文件夹下分两类：因为我已经正好在该文件夹下分了两个文件夹\n",
    ")\n",
    "\n",
    "# 正式创建图片的生成器：test\n",
    "test_generator = test_datagen.flow_from_directory( test_dir,                  # 待读取文件的目录\n",
    "                                                    target_size = (200,200),    # 图片的统一大小 \n",
    "                                                    batch_size = 20,            # 每次读入20张\n",
    "                                                    class_mode = 'binary'       # 该文件夹下分两类：因为我已经正好在该文件夹下分了两个文件夹\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络搭建：和26中完全一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入keras内置的VGG16网络：\n",
    "# weights参数：使用训练imagenet后得到的权重参数；若不设置该参数，则默认只使用VGG16的“层结构”\n",
    "# include_top参数：是否引入VGG16自带的分类器\n",
    "conv_base = keras.applications.VGG16( weights = 'imagenet', include_top = False)\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "# 添加全连接层：\n",
    "model = keras.Sequential()\n",
    "model.add( conv_base )  # 把卷积基先加入：一个网络可以直接当作另外一个模型中的某些层，非常方便！\n",
    "model.add( layers.GlobalAveragePooling2D() )  # 与layers.Flatten()功能一样，它的效率更高（这两年才出的）\n",
    "model.add( layers.Dense(512, activation='relu') )\n",
    "model.add( layers.Dense(1, activation='relu') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( optimizer='adam',\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此时输入数据是“generator生成器”，所以用model.fit_generator\n",
    "history = model.fit_generator( \n",
    "     # 训练数据：\n",
    "     train_generator,\n",
    "     epochs = 2,\n",
    "     steps_per_epoch = 100,  # 有2000张图，每次录入20张，因此100步可以走完一个epoch！不设置的话，生成器会一直循环生成\n",
    "     # 测试数据：\n",
    "     validation_data = test_generator, \n",
    "     validation_steps = 50   # 有1000张图，每次录入20张，故50步即可走完一个epoch！\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当使用Xception这样的大网络时，使用“数据增强”就很不好！因为它导致所以的卷积层都要参与到“重复卷积计算”当中！这是非常非常耗时的\n",
    "（卷积层本来就十分耗时，现在卷积基又是这么大！）。\n",
    "\n",
    "因此：我偏向于使用29种的那套操作流程：\n",
    "- 使用Xception预训练网络：特征提取卷积基只用一次，训练Dense层的参数；\n",
    "- 卷积基顶部几层卷积层解冻，进行微调。—— 至此，肯定比直接用预训练网络后，比自己搭建的CNN网格更好！\n",
    "- 若这样的结果都不满足：保存上一步微调后的整个卷积基的权重参数，对训练数据进行数据增强，然后重回第一步但特征不提取！\n",
    "\n",
    "说明：网络结果不好，最大的根源是训练数据不够！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
